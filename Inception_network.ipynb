{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fn1xbKkNnLRl"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# -----------------------\n",
        "# Reproducibility helpers\n",
        "# -----------------------\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    cudnn.deterministic = False\n",
        "    cudnn.benchmark = True  # faster on GPUs for conv nets\n",
        "\n",
        "# -----------------------\n",
        "# Data\n",
        "# -----------------------\n",
        "def get_cifar10_loaders(batch_size: int = 128, num_workers: int = 4) -> Tuple[DataLoader, DataLoader]:\n",
        "    mean = (0.4914, 0.4822, 0.4465)\n",
        "    std  = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "    train_tf = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "    test_tf = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "    train_ds = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=train_tf)\n",
        "    test_ds  = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=test_tf)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=num_workers, pin_memory=True)\n",
        "    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# Train / Eval\n",
        "# -----------------------\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    total, correct, running_loss = 0, 0, 0.0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    return running_loss / total, correct / total\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, device):\n",
        "    model.train()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    return running_loss / total, correct / total"
      ],
      "metadata": {
        "id": "FkOAzqR2naXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# Model: Inception for CIFAR-10\n",
        "# -----------------------\n",
        "class ConvBNReLU(nn.Module):\n",
        "    def __init__(self, in_c, out_c, k=3, s=1, p=1):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_c, out_c, k, s, p, bias=False)\n",
        "        self.bn   = nn.BatchNorm2d(out_c)\n",
        "        self.act  = nn.ReLU(inplace=True)\n",
        "    def forward(self, x):\n",
        "        return self.act(self.bn(self.conv(x)))\n",
        "\n",
        "class InceptionBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    v1-style block:\n",
        "      - b1: 1x1\n",
        "      - b2: 1x1 -> 3x3\n",
        "      - b3: 1x1 -> 5x5 (implemented as 3x3 -> 3x3)\n",
        "      - b4: 3x3 pool -> 1x1\n",
        "    \"\"\"\n",
        "    def __init__(self, in_c, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, pool_proj,\n",
        "                 pool_type=\"avg\"):\n",
        "        super().__init__()\n",
        "        # 1x1\n",
        "        self.b1 = ConvBNReLU(in_c, out_1x1, k=1, p=0)\n",
        "\n",
        "        # 1x1 -> 3x3\n",
        "        self.b2_reduce = ConvBNReLU(in_c, red_3x3, k=1, p=0)\n",
        "        self.b2_conv   = ConvBNReLU(red_3x3, out_3x3, k=3, p=1)\n",
        "\n",
        "        # 1x1 -> (3x3 -> 3x3) â‰ˆ 5x5\n",
        "        self.b3_reduce = ConvBNReLU(in_c, red_5x5, k=1, p=0)\n",
        "        self.b3_conv1  = ConvBNReLU(red_5x5, out_5x5, k=3, p=1)\n",
        "        self.b3_conv2  = ConvBNReLU(out_5x5, out_5x5, k=3, p=1)\n",
        "\n",
        "        # pool -> 1x1\n",
        "        if pool_type == \"avg\":\n",
        "            self.pool = nn.AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
        "        else:\n",
        "            self.pool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
        "        self.b4_proj = ConvBNReLU(in_c, pool_proj, k=1, p=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b1 = self.b1(x)\n",
        "\n",
        "        r2 = self.b2_reduce(x)\n",
        "        b2 = self.b2_conv(r2)\n",
        "\n",
        "        r3 = self.b3_reduce(x)\n",
        "        b3 = self.b3_conv2(self.b3_conv1(r3))\n",
        "\n",
        "        b4 = self.b4_proj(self.pool(x))\n",
        "\n",
        "        return torch.cat([b1, b2, b3, b4], dim=1)\n",
        "\n",
        "class InceptionNetCIFAR(nn.Module):\n",
        "    \"\"\"\n",
        "    A slim GoogLeNet-v1 style network sized for 32x32 CIFAR-10.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=10, dropout=0.2):\n",
        "        super().__init__()\n",
        "        # Stem (keep strides=1 for 32x32)\n",
        "        self.stem = nn.Sequential(\n",
        "            ConvBNReLU(3, 64, k=3, s=1, p=1),\n",
        "            ConvBNReLU(64, 64, k=3, s=1, p=1),\n",
        "            nn.MaxPool2d(3, stride=2, padding=1),  # 32->16\n",
        "        )\n",
        "\n",
        "        # Inception stack (channels tuned for small model)\n",
        "        # After stem: 64ch\n",
        "        self.inc1 = InceptionBlock(\n",
        "            in_c=64,\n",
        "            out_1x1=32,\n",
        "            red_3x3=32, out_3x3=48,\n",
        "            red_5x5=8,  out_5x5=16,\n",
        "            pool_proj=16,\n",
        "            pool_type=\"avg\"\n",
        "        )  # -> 32+48+16+16 = 112 ch\n",
        "\n",
        "        self.down1 = nn.MaxPool2d(3, stride=2, padding=1)  # 16->8\n",
        "\n",
        "        self.inc2 = InceptionBlock(\n",
        "            in_c=112,\n",
        "            out_1x1=64,\n",
        "            red_3x3=48, out_3x3=64,\n",
        "            red_5x5=16, out_5x5=32,\n",
        "            pool_proj=32,\n",
        "            pool_type=\"avg\"\n",
        "        )  # -> 64+64+32+32 = 192 ch\n",
        "\n",
        "        self.inc3 = InceptionBlock(\n",
        "            in_c=192,\n",
        "            out_1x1=96,\n",
        "            red_3x3=64, out_3x3=96,\n",
        "            red_5x5=24, out_5x5=64,\n",
        "            pool_proj=64,\n",
        "            pool_type=\"avg\"\n",
        "        )  # -> 96+96+64+64 = 320 ch\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.AdaptiveAvgPool2d(1),  # 8x8 -> 1x1\n",
        "        )\n",
        "        self.fc = nn.Linear(320, num_classes)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1.)\n",
        "                nn.init.constant_(m.bias, 0.)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0.)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.inc1(x)\n",
        "        x = self.down1(x)\n",
        "        x = self.inc2(x)\n",
        "        x = self.inc3(x)\n",
        "        x = self.head(x)           # (B, 320, 1, 1)\n",
        "        x = torch.flatten(x, 1)    # (B, 320)\n",
        "        return self.fc(x)\n",
        "\n",
        "\n",
        "class AlexNetCIFAR(nn.Module):\n",
        "    \"\"\"\n",
        "    AlexNet adapted for 32x32 inputs:\n",
        "    - Use 3x3 convs (stride 1) instead of 11x11/5x5\n",
        "    - Slightly reduced channels to fit CIFAR-10 scale\n",
        "    - Removing the FC layers to use a pointwise convolution and Global Average Pooling\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes: int = 10, dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),  # 32x32 -> 32x32\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),                  # 32 -> 16\n",
        "\n",
        "            nn.Conv2d(64, 192, kernel_size=3, padding=1),           # 16 -> 16\n",
        "            nn.BatchNorm2d(192),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),                  # 16 -> 8\n",
        "\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),          # 8 -> 8\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),          # 8 -> 8\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),          # 8 -> 8\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),                  # 8 -> 4\n",
        "        )\n",
        "\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Conv2d(256, num_classes, kernel_size=1),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        return self.head(x)"
      ],
      "metadata": {
        "id": "QHj61NLn9RzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AlexNetCIFAR()\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters in AlexNet: {total_params}\")\n",
        "\n",
        "model = InceptionNetCIFAR()\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters in InceptionNet: {total_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imnmCdFj-Uu1",
        "outputId": "85e9ea50-62c5-4fe6-b43f-b7d839aea163"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters in AlexNet: 2256458\n",
            "Total number of parameters in InceptionNet: 279818\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "batch_size = 128\n",
        "lr = 5e-4\n",
        "weight_decay = 5e-4\n",
        "num_workers = 4\n",
        "seed = 42"
      ],
      "metadata": {
        "id": "lLwq8AHq9xFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(seed)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "train_loader, test_loader = get_cifar10_loaders(batch_size=batch_size, num_workers=num_workers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h98GaXn498TB",
        "outputId": "506edb19-69be-4a66-8f42-010f1155d66e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170M/170M [00:03<00:00, 48.6MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(model_name='InceptionNet', num_classes=10):\n",
        "  model = InceptionNetCIFAR(num_classes=num_classes)\n",
        "  model.to(device)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "  best_acc = 0.0\n",
        "  for epoch in range(1, epochs + 1):\n",
        "      train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, device)\n",
        "      val_loss,  val_acc  = evaluate(model, test_loader, device)\n",
        "\n",
        "      if val_acc > best_acc:\n",
        "          best_acc = val_acc\n",
        "          os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "          torch.save({\"model\": model.state_dict(),\n",
        "                      \"epoch\": epoch,\n",
        "                      \"acc\": best_acc\n",
        "                      },\n",
        "                      f\"checkpoints/{model_name}_best.pt\")\n",
        "\n",
        "      print(f\"Epoch {epoch:02d}/{epochs} | \"\n",
        "            f\"Train Loss {train_loss:.4f} Acc {train_acc*100:.2f}% | \"\n",
        "            f\"Val Loss {val_loss:.4f} Acc {val_acc*100:.2f}% | \"\n",
        "            f\"Best Val Acc {best_acc*100:.2f}%\")\n",
        "\n",
        "  # Final evaluation\n",
        "  test_loss, test_acc = evaluate(model, test_loader, device)\n",
        "  print(f\"\\nFinal {model_name.upper()} Test Accuracy: {test_acc*100:.2f}% (loss {test_loss:.4f})\")"
      ],
      "metadata": {
        "id": "7RqFqBpd89Wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-apR1qVM_kgg",
        "outputId": "80ea6ac6-0835-4420-c083-167ba65cdb5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01/10 | Train Loss 1.4470 Acc 46.93% | Val Loss 1.1514 Acc 57.68% | Best Val Acc 57.68%\n",
            "Epoch 02/10 | Train Loss 1.0420 Acc 62.73% | Val Loss 0.9709 Acc 65.36% | Best Val Acc 65.36%\n",
            "Epoch 03/10 | Train Loss 0.8906 Acc 68.29% | Val Loss 0.8735 Acc 69.10% | Best Val Acc 69.10%\n",
            "Epoch 04/10 | Train Loss 0.7869 Acc 72.24% | Val Loss 0.7886 Acc 72.50% | Best Val Acc 72.50%\n",
            "Epoch 05/10 | Train Loss 0.7167 Acc 75.01% | Val Loss 0.7514 Acc 74.11% | Best Val Acc 74.11%\n",
            "Epoch 06/10 | Train Loss 0.6651 Acc 76.85% | Val Loss 0.7387 Acc 74.87% | Best Val Acc 74.87%\n",
            "Epoch 07/10 | Train Loss 0.6177 Acc 78.60% | Val Loss 0.6703 Acc 76.89% | Best Val Acc 76.89%\n",
            "Epoch 08/10 | Train Loss 0.5781 Acc 79.99% | Val Loss 0.6868 Acc 76.63% | Best Val Acc 76.89%\n",
            "Epoch 09/10 | Train Loss 0.5561 Acc 80.75% | Val Loss 0.5807 Acc 80.27% | Best Val Acc 80.27%\n",
            "Epoch 10/10 | Train Loss 0.5316 Acc 81.56% | Val Loss 0.6179 Acc 78.99% | Best Val Acc 80.27%\n",
            "\n",
            "Final INCEPTIONNET Test Accuracy: 78.99% (loss 0.6179)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "Inception network inspired architecture we used here has far fewer parameters than other networks like AlexNet, yet can achieve comparable if not better performance on the same dataset with similar parameters.\n",
        "\n",
        "We can see this stark contrast to other networks trained [here](https://github.com/ajhalthor/computer-vision-101/blob/main/pointwise_convolution/pointwise_convolutions.ipynb)"
      ],
      "metadata": {
        "id": "ICdGswwfB-V9"
      }
    }
  ]
}