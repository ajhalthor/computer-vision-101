{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LFFbaPCL9eem"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# -----------------------\n",
        "# Reproducibility helpers\n",
        "# -----------------------\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    cudnn.deterministic = False\n",
        "    cudnn.benchmark = True  # faster on GPUs for conv nets\n",
        "\n",
        "# -----------------------\n",
        "# Data\n",
        "# -----------------------\n",
        "def get_cifar10_loaders(batch_size: int = 128, num_workers: int = 4) -> Tuple[DataLoader, DataLoader]:\n",
        "    mean = (0.4914, 0.4822, 0.4465)\n",
        "    std  = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "    train_tf = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "    test_tf = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "    train_ds = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=train_tf)\n",
        "    test_ds  = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=test_tf)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=num_workers, pin_memory=True)\n",
        "    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# Train / Eval\n",
        "# -----------------------\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    total, correct, running_loss = 0, 0, 0.0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    return running_loss / total, correct / total\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, device):\n",
        "    model.train()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    return running_loss / total, correct / total"
      ],
      "metadata": {
        "id": "j94Zyyv89k9D"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNetCIFAR(nn.Module):\n",
        "    \"\"\"\n",
        "    AlexNet adapted for 32x32 inputs:\n",
        "    - Use 3x3 convs (stride 1) instead of 11x11/5x5\n",
        "    - Slightly reduced channels to fit CIFAR-10 scale\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes: int = 10, dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=7, stride=1, padding=1),  # 32x32 -> 32x32\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),                  # 32 -> 16\n",
        "\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=1),           # 16 -> 16\n",
        "            nn.BatchNorm2d(192),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),                  # 16 -> 8\n",
        "\n",
        "            nn.Conv2d(192, 384, kernel_size=5, padding=1),          # 8 -> 8\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),          # 8 -> 8\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),          # 8 -> 8\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),                  # 8 -> 4\n",
        "        )\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Conv2d(256, num_classes, kernel_size=1),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        return self.head(x)"
      ],
      "metadata": {
        "id": "KlticlArA8f8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VGGNetCIFAR(nn.Module):\n",
        "    \"\"\"\n",
        "    Replace\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes: int = 10, dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1),  # 32x32 -> 32x32\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),  # 32x32 -> 32x32\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(16, 64, kernel_size=3, stride=1, padding=1),  # 32x32 -> 32x32\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),                  # 32 -> 16\n",
        "\n",
        "            nn.Conv2d(64, 32, kernel_size=3, padding=1),           # 16 -> 16\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 192, kernel_size=3, padding=1),           # 16 -> 16\n",
        "            nn.BatchNorm2d(192),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),                  # 16 -> 8\n",
        "\n",
        "            nn.Conv2d(192, 64, kernel_size=3, padding=1),          # 8 -> 8\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 384, kernel_size=3, padding=1),          # 8 -> 8\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),          # 8 -> 8\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),          # 8 -> 8\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),                  # 8 -> 4\n",
        "        )\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Conv2d(256, num_classes, kernel_size=1),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        return self.head(x)"
      ],
      "metadata": {
        "id": "uy-GDqmPC8Oq"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AlexNetCIFAR()\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters in ALEXNET: {total_params}\")\n",
        "\n",
        "model = VGGNetCIFAR()\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters in VGGNet: {total_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2id4JU_n-OsH",
        "outputId": "eacade31-2ab0-490b-b18c-f5dbfb0e8d8c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters in ALEXNET: 3639882\n",
            "Total number of parameters in VGGNet: 1896522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "batch_size = 128\n",
        "lr = 0.001\n",
        "weight_decay = 5e-4\n",
        "num_workers = 2\n",
        "seed = 42"
      ],
      "metadata": {
        "id": "LnzpMxDK-kDX"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(seed)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "train_loader, test_loader = get_cifar10_loaders(batch_size=batch_size, num_workers=num_workers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQpODCCB-pIs",
        "outputId": "bcaa9aa3-0edc-4799-fd6d-25d04a4c1c31"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 41.6MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(model_name, num_classes=10):\n",
        "  if model_name == 'vggnet':\n",
        "    model = VGGNetCIFAR()\n",
        "  else:\n",
        "    model = AlexNetCIFAR()\n",
        "\n",
        "  model.to(device)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "  best_acc = 0.0\n",
        "  for epoch in range(1, epochs + 1):\n",
        "      train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, device)\n",
        "      val_loss,  val_acc  = evaluate(model, test_loader, device)\n",
        "\n",
        "      if val_acc > best_acc:\n",
        "          best_acc = val_acc\n",
        "          os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "          torch.save({\"model\": model.state_dict(),\n",
        "                      \"epoch\": epoch,\n",
        "                      \"acc\": best_acc\n",
        "                      },\n",
        "                      f\"checkpoints/{model_name}_best.pt\")\n",
        "\n",
        "      print(f\"Epoch {epoch:02d}/{epochs} | \"\n",
        "            f\"Train Loss {train_loss:.4f} Acc {train_acc*100:.2f}% | \"\n",
        "            f\"Val Loss {val_loss:.4f} Acc {val_acc*100:.2f}% | \"\n",
        "            f\"Best Val Acc {best_acc*100:.2f}%\")\n",
        "\n",
        "  # Final evaluation\n",
        "  test_loss, test_acc = evaluate(model, test_loader, device)\n",
        "  print(f\"\\nFinal {model_name.upper()} Test Accuracy: {test_acc*100:.2f}% (loss {test_loss:.4f})\")"
      ],
      "metadata": {
        "id": "l_WkyPCU-rQO"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_evaluate(model_name='alexnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3picGhgcBZoQ",
        "outputId": "9a2f886a-7276-4d37-a993-ac346569fef0"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01/10 | Train Loss 1.4239 Acc 47.82% | Val Loss 1.4908 Acc 49.50% | Best Val Acc 49.50%\n",
            "Epoch 02/10 | Train Loss 1.0814 Acc 61.55% | Val Loss 0.9764 Acc 66.05% | Best Val Acc 66.05%\n",
            "Epoch 03/10 | Train Loss 0.9357 Acc 67.13% | Val Loss 0.9969 Acc 66.08% | Best Val Acc 66.08%\n",
            "Epoch 04/10 | Train Loss 0.8378 Acc 70.73% | Val Loss 0.8527 Acc 70.26% | Best Val Acc 70.26%\n",
            "Epoch 05/10 | Train Loss 0.7738 Acc 72.99% | Val Loss 0.7195 Acc 75.41% | Best Val Acc 75.41%\n",
            "Epoch 06/10 | Train Loss 0.7237 Acc 75.10% | Val Loss 0.7873 Acc 73.21% | Best Val Acc 75.41%\n",
            "Epoch 07/10 | Train Loss 0.6834 Acc 76.06% | Val Loss 0.8066 Acc 73.52% | Best Val Acc 75.41%\n",
            "Epoch 08/10 | Train Loss 0.6482 Acc 77.61% | Val Loss 0.7547 Acc 75.25% | Best Val Acc 75.41%\n",
            "Epoch 09/10 | Train Loss 0.6223 Acc 78.45% | Val Loss 0.6679 Acc 77.84% | Best Val Acc 77.84%\n",
            "Epoch 10/10 | Train Loss 0.5892 Acc 79.55% | Val Loss 0.6753 Acc 77.54% | Best Val Acc 77.84%\n",
            "\n",
            "Final ALEXNET Test Accuracy: 77.54% (loss 0.6753)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_evaluate(model_name='vggnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzijLsWsEvVR",
        "outputId": "aba8f2cb-35be-4106-8126-f4cc208eb322"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01/10 | Train Loss 1.4492 Acc 46.38% | Val Loss 1.2956 Acc 53.46% | Best Val Acc 53.46%\n",
            "Epoch 02/10 | Train Loss 1.0078 Acc 63.93% | Val Loss 1.0559 Acc 64.58% | Best Val Acc 64.58%\n",
            "Epoch 03/10 | Train Loss 0.8272 Acc 70.70% | Val Loss 1.1721 Acc 62.95% | Best Val Acc 64.58%\n",
            "Epoch 04/10 | Train Loss 0.7122 Acc 75.27% | Val Loss 0.7277 Acc 75.17% | Best Val Acc 75.17%\n",
            "Epoch 05/10 | Train Loss 0.6487 Acc 77.58% | Val Loss 0.8033 Acc 72.76% | Best Val Acc 75.17%\n",
            "Epoch 06/10 | Train Loss 0.5997 Acc 79.48% | Val Loss 0.7025 Acc 77.10% | Best Val Acc 77.10%\n",
            "Epoch 07/10 | Train Loss 0.5573 Acc 80.76% | Val Loss 0.6816 Acc 77.59% | Best Val Acc 77.59%\n",
            "Epoch 08/10 | Train Loss 0.5205 Acc 82.14% | Val Loss 0.5812 Acc 80.58% | Best Val Acc 80.58%\n",
            "Epoch 09/10 | Train Loss 0.4951 Acc 83.03% | Val Loss 0.6399 Acc 79.63% | Best Val Acc 80.58%\n",
            "Epoch 10/10 | Train Loss 0.4674 Acc 84.05% | Val Loss 0.6458 Acc 79.79% | Best Val Acc 80.58%\n",
            "\n",
            "Final VGGNET Test Accuracy: 79.79% (loss 0.6458)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "Replacing 7x7 and 5x5 convolutions with 3x3 convolutions, the network has 2x less parameters while maintaining better performance than AlexNet."
      ],
      "metadata": {
        "id": "E-dQf2sNRgvj"
      }
    }
  ]
}