{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3mdqVDPwnXz7"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# -----------------------\n",
        "# Reproducibility helpers\n",
        "# -----------------------\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    cudnn.deterministic = False\n",
        "    cudnn.benchmark = True  # faster on GPUs for conv nets\n",
        "\n",
        "# -----------------------\n",
        "# Data\n",
        "# -----------------------\n",
        "def get_cifar10_loaders(batch_size: int = 128, num_workers: int = 4) -> Tuple[DataLoader, DataLoader]:\n",
        "    mean = (0.4914, 0.4822, 0.4465)\n",
        "    std  = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "    train_tf = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "    test_tf = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "    train_ds = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=train_tf)\n",
        "    test_ds  = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=test_tf)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=num_workers, pin_memory=True)\n",
        "    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNetPlainCIFAR(nn.Module):\n",
        "    \"\"\"\n",
        "    AlexNet adapted for 32x32 inputs:\n",
        "    - Use 3x3 convs (stride 1) instead of 11x11/5x5\n",
        "    - Slightly reduced channels to fit CIFAR-10 scale\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes: int = 10, dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),  # 32x32 -> 32x32\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),                  # 32 -> 16\n",
        "\n",
        "            nn.Conv2d(64, 192, kernel_size=3, padding=1),           # 16 -> 16\n",
        "            nn.BatchNorm2d(192),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),                  # 16 -> 8\n",
        "\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),          # 8 -> 8\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),          # 8 -> 8\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),          # 8 -> 8\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),                  # 8 -> 4\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(256 * 4 * 4, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "class AlexNetCIFAR(nn.Module):\n",
        "    \"\"\"\n",
        "    AlexNet adapted for 32x32 inputs:\n",
        "    - Use 3x3 convs (stride 1) instead of 11x11/5x5\n",
        "    - Slightly reduced channels to fit CIFAR-10 scale\n",
        "    - Removing the FC layers to use a pointwise convolution and Global Average Pooling\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes: int = 10, dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),  # 32x32 -> 32x32\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),                  # 32 -> 16\n",
        "\n",
        "            nn.Conv2d(64, 192, kernel_size=3, padding=1),           # 16 -> 16\n",
        "            nn.BatchNorm2d(192),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),                  # 16 -> 8\n",
        "\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),          # 8 -> 8\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),          # 8 -> 8\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),          # 8 -> 8\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),                  # 8 -> 4\n",
        "        )\n",
        "\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Conv2d(256, num_classes, kernel_size=1),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        return self.head(x)\n",
        "\n",
        "class NiNCIFAR(nn.Module):\n",
        "    \"\"\"\n",
        "    AlexNet modified to include pointwise convolutions throughout the network.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes: int = 10, dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, kernel_size=1, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 192, kernel_size=1, stride=1, padding=1), # expand (ensuring the 128 x 192 uses pointwise convolution)\n",
        "            nn.BatchNorm2d(192),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(192, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Conv2d(256, 256, kernel_size=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Conv2d(256, 384, kernel_size=1, padding=1), # expand\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(384, 256, kernel_size=1, padding=1),  # contract (ensuring the 384 x 256 uses pointwise convolution)\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Conv2d(256, 256, kernel_size=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Conv2d(256, num_classes, kernel_size=1), # contract\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        return self.head(x)"
      ],
      "metadata": {
        "id": "2azId1NSDvAF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AlexNetPlainCIFAR()\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters in ALEXNET (plain): {total_params}\")\n",
        "\n",
        "model = AlexNetCIFAR()\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters in ALEXNET (no FC layers): {total_params}\")\n",
        "\n",
        "model = NiNCIFAR()\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters in NiN: {total_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e0kThmfG8BE",
        "outputId": "b5a617ef-247e-4e71-e35c-c064a3f65dc3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters in ALEXNET (plain): 6979146\n",
            "Total number of parameters in ALEXNET (no FC layers): 2256458\n",
            "Total number of parameters in NiN: 1485386\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# Train / Eval\n",
        "# -----------------------\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    total, correct, running_loss = 0, 0, 0.0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    return running_loss / total, correct / total\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, device):\n",
        "    model.train()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    return running_loss / total, correct / total"
      ],
      "metadata": {
        "id": "oDl6e38inYw1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "batch_size = 126\n",
        "lr = 0.0005\n",
        "weight_decay = 5e-4\n",
        "seed = 42\n",
        "\n",
        "set_seed()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "train_loader, test_loader = get_cifar10_loaders(batch_size=batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0cJ9VRHHzUD",
        "outputId": "c2660a4f-b002-411e-cb1e-fd1456c78ec2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 77.5MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(model_name='alexnet'):\n",
        "  if model_name == \"alexnet\":\n",
        "    model = AlexNetCIFAR()\n",
        "  elif model_name == 'alexnet_plain':\n",
        "    model = AlexNetPlainCIFAR()\n",
        "  else:\n",
        "    model = NiNCIFAR()\n",
        "\n",
        "  model.to(device)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=weight_decay)\n",
        "\n",
        "  best_acc = 0.0\n",
        "  for epoch in range(1, epochs + 1):\n",
        "      train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, device)\n",
        "      val_loss,  val_acc  = evaluate(model, test_loader, device)\n",
        "\n",
        "      if val_acc > best_acc:\n",
        "          best_acc = val_acc\n",
        "          os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "          torch.save({\"model\": model.state_dict(),\n",
        "                      \"epoch\": epoch,\n",
        "                      \"acc\": best_acc\n",
        "                      },\n",
        "                      f\"checkpoints/{model_name}_best.pt\")\n",
        "\n",
        "      print(f\"Epoch {epoch:02d}/{epochs} | \"\n",
        "            f\"Train Loss {train_loss:.4f} Acc {train_acc*100:.2f}% | \"\n",
        "            f\"Val Loss {val_loss:.4f} Acc {val_acc*100:.2f}% | \"\n",
        "            f\"Best Val Acc {best_acc*100:.2f}%\")\n",
        "\n",
        "  # Final evaluation\n",
        "  test_loss, test_acc = evaluate(model, test_loader, device)\n",
        "  print(f\"\\nFinal {model_name.upper()} Test Accuracy: {test_acc*100:.2f}% (loss {test_loss:.4f})\")"
      ],
      "metadata": {
        "id": "JzXsS6BCH59F"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_evaluate(model_name='alexnet_plain')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAes9xnyzbbV",
        "outputId": "e200b438-3cba-4f81-f956-f86463d6f009"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01/10 | Train Loss 1.5478 Acc 42.47% | Val Loss 1.2026 Acc 56.20% | Best Val Acc 56.20%\n",
            "Epoch 02/10 | Train Loss 1.1516 Acc 58.80% | Val Loss 1.0272 Acc 63.28% | Best Val Acc 63.28%\n",
            "Epoch 03/10 | Train Loss 0.9975 Acc 64.88% | Val Loss 1.0372 Acc 63.49% | Best Val Acc 63.49%\n",
            "Epoch 04/10 | Train Loss 0.9002 Acc 68.46% | Val Loss 0.8523 Acc 71.69% | Best Val Acc 71.69%\n",
            "Epoch 05/10 | Train Loss 0.8167 Acc 71.88% | Val Loss 0.9406 Acc 70.55% | Best Val Acc 71.69%\n",
            "Epoch 06/10 | Train Loss 0.7516 Acc 74.48% | Val Loss 0.7157 Acc 75.55% | Best Val Acc 75.55%\n",
            "Epoch 07/10 | Train Loss 0.7014 Acc 76.37% | Val Loss 0.6688 Acc 77.41% | Best Val Acc 77.41%\n",
            "Epoch 08/10 | Train Loss 0.6614 Acc 77.89% | Val Loss 0.6266 Acc 79.05% | Best Val Acc 79.05%\n",
            "Epoch 09/10 | Train Loss 0.6185 Acc 79.45% | Val Loss 0.6859 Acc 77.80% | Best Val Acc 79.05%\n",
            "Epoch 10/10 | Train Loss 0.5916 Acc 80.16% | Val Loss 0.6424 Acc 78.58% | Best Val Acc 79.05%\n",
            "\n",
            "Final ALEXNET_PLAIN Test Accuracy: 78.58% (loss 0.6424)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_evaluate(model_name='alexnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAB6MnHYIQJG",
        "outputId": "1712ca7a-2ef0-47b7-f1b4-dafbafabf4fa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01/10 | Train Loss 1.3292 Acc 51.58% | Val Loss 1.5289 Acc 48.95% | Best Val Acc 48.95%\n",
            "Epoch 02/10 | Train Loss 0.9533 Acc 66.22% | Val Loss 0.9590 Acc 66.35% | Best Val Acc 66.35%\n",
            "Epoch 03/10 | Train Loss 0.7942 Acc 72.14% | Val Loss 1.0031 Acc 65.33% | Best Val Acc 66.35%\n",
            "Epoch 04/10 | Train Loss 0.7082 Acc 75.37% | Val Loss 0.9508 Acc 68.23% | Best Val Acc 68.23%\n",
            "Epoch 05/10 | Train Loss 0.6496 Acc 77.54% | Val Loss 0.9267 Acc 70.45% | Best Val Acc 70.45%\n",
            "Epoch 06/10 | Train Loss 0.5981 Acc 79.37% | Val Loss 0.6574 Acc 77.09% | Best Val Acc 77.09%\n",
            "Epoch 07/10 | Train Loss 0.5586 Acc 80.80% | Val Loss 0.8094 Acc 73.58% | Best Val Acc 77.09%\n",
            "Epoch 08/10 | Train Loss 0.5263 Acc 81.88% | Val Loss 0.6889 Acc 76.89% | Best Val Acc 77.09%\n",
            "Epoch 09/10 | Train Loss 0.4966 Acc 83.04% | Val Loss 0.6744 Acc 78.41% | Best Val Acc 78.41%\n",
            "Epoch 10/10 | Train Loss 0.4738 Acc 83.71% | Val Loss 0.5613 Acc 81.14% | Best Val Acc 81.14%\n",
            "\n",
            "Final ALEXNET Test Accuracy: 81.14% (loss 0.5613)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_evaluate(model_name='NiN')"
      ],
      "metadata": {
        "id": "c98taiYPIS9w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a237115c-dbc9-4e11-8e99-74bd6b3a2a38"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01/10 | Train Loss 1.4436 Acc 47.06% | Val Loss 1.4685 Acc 47.14% | Best Val Acc 47.14%\n",
            "Epoch 02/10 | Train Loss 1.1525 Acc 58.51% | Val Loss 1.2084 Acc 57.39% | Best Val Acc 57.39%\n",
            "Epoch 03/10 | Train Loss 1.0611 Acc 62.12% | Val Loss 1.0820 Acc 61.19% | Best Val Acc 61.19%\n",
            "Epoch 04/10 | Train Loss 0.9811 Acc 64.91% | Val Loss 1.0550 Acc 62.91% | Best Val Acc 62.91%\n",
            "Epoch 05/10 | Train Loss 0.9309 Acc 67.10% | Val Loss 0.9825 Acc 65.69% | Best Val Acc 65.69%\n",
            "Epoch 06/10 | Train Loss 0.8748 Acc 69.26% | Val Loss 0.9450 Acc 68.04% | Best Val Acc 68.04%\n",
            "Epoch 07/10 | Train Loss 0.8292 Acc 70.85% | Val Loss 0.8811 Acc 69.52% | Best Val Acc 69.52%\n",
            "Epoch 08/10 | Train Loss 0.7979 Acc 72.09% | Val Loss 1.0280 Acc 65.16% | Best Val Acc 69.52%\n",
            "Epoch 09/10 | Train Loss 0.7637 Acc 73.49% | Val Loss 0.9304 Acc 67.89% | Best Val Acc 69.52%\n",
            "Epoch 10/10 | Train Loss 0.7444 Acc 74.03% | Val Loss 0.7516 Acc 74.27% | Best Val Acc 74.27%\n",
            "\n",
            "Final NIN Test Accuracy: 74.27% (loss 0.7516)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "- During convolution operations, large number of filters appear in deeper layers. Thus each additional convolution layer becomes expensive as its adds potentially millions of parameters to the network.\n",
        "- With pointwise convolutions, we are able to control the channel depth at any point in the network without adding too many learnable parameters to the network.\n",
        "- In the example above, we control the number of parameters in the network to drop by 30% compared to AlexNet (5x compared to AlexNet plain). Yet, training the exact same amount of time on the same data yeilds comparable performance."
      ],
      "metadata": {
        "id": "g09YcvFnP-SC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZRlcS-Z4Q_hm"
      }
    }
  ]
}