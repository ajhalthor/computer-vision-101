{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "L2O-MRg2sN0i",
        "omjjDhqnsjjY",
        "gR94bKW8s5Ly",
        "-_xFApvYt1DH",
        "epoWUq8et9QI",
        "MHKBaRVy_Xxp"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YcLtye_Xrh90"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# -----------------------\n",
        "# Reproducibility helpers\n",
        "# -----------------------\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    cudnn.deterministic = False\n",
        "    cudnn.benchmark = True  # faster on GPUs for conv nets\n",
        "\n",
        "# -----------------------\n",
        "# Data\n",
        "# -----------------------\n",
        "def get_cifar10_loaders(batch_size: int = 128, num_workers: int = 4) -> Tuple[DataLoader, DataLoader]:\n",
        "    mean = (0.4914, 0.4822, 0.4465)\n",
        "    std  = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "    train_tf = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "    test_tf = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "    train_ds = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=train_tf)\n",
        "    test_ds  = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=test_tf)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=num_workers, pin_memory=True)\n",
        "    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# Train / Eval\n",
        "# -----------------------\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    total, correct, running_loss = 0, 0, 0.0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    return running_loss / total, correct / total\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, device):\n",
        "    model.train()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    return running_loss / total, correct / total"
      ],
      "metadata": {
        "id": "2LQJVfUHr-7Y"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(model_name, num_classes=10):\n",
        "  if model_name == 'alexnet':\n",
        "    model = AlexNetCIFAR()\n",
        "  elif model_name == 'vggnet':\n",
        "    model = VGGNetCIFAR()\n",
        "  elif model_name == 'vggnet_batch_norm':\n",
        "    model = VGGNetBNCIFAR()\n",
        "  elif model_name == 'deep_vggnet':\n",
        "    model = DeepVGGNetCIFAR()\n",
        "  elif model_name == 'resnet':\n",
        "    model = ResNetCIFAR()\n",
        "  else:\n",
        "    model = ResNetV2CIFAR()\n",
        "\n",
        "  model.to(device)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "  best_acc = 0.0\n",
        "  for epoch in range(1, epochs + 1):\n",
        "      train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, device)\n",
        "      val_loss,  val_acc  = evaluate(model, test_loader, device)\n",
        "\n",
        "      if val_acc > best_acc:\n",
        "          best_acc = val_acc\n",
        "          os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "          torch.save({\"model\": model.state_dict(),\n",
        "                      \"epoch\": epoch,\n",
        "                      \"acc\": best_acc\n",
        "                      },\n",
        "                      f\"checkpoints/{model_name}_best.pt\")\n",
        "\n",
        "      print(f\"Epoch {epoch:02d}/{epochs} | \"\n",
        "            f\"Train Loss {train_loss:.4f} Acc {train_acc*100:.2f}% | \"\n",
        "            f\"Val Loss {val_loss:.4f} Acc {val_acc*100:.2f}% | \"\n",
        "            f\"Best Val Acc {best_acc*100:.2f}%\")\n",
        "\n",
        "  # Final evaluation\n",
        "  test_loss, test_acc = evaluate(model, test_loader, device)\n",
        "  print(f\"\\nFinal {model_name.upper()} Test Accuracy: {test_acc*100:.2f}% (loss {test_loss:.4f})\")"
      ],
      "metadata": {
        "id": "WDsq6gmKsynN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ActStats:\n",
        "    \"Testing for the dying relu problem\"\n",
        "    def __init__(self, model):\n",
        "        self.handles = []\n",
        "        self.stats = OrderedDict()\n",
        "        idx = 0\n",
        "        for name, m in model.named_modules():\n",
        "            # focus on conv/linear/activation outputs (skip containers)\n",
        "            if isinstance(m, (torch.nn.Conv2d, torch.nn.Linear, torch.nn.ReLU)):\n",
        "                key = f\"{name or 'model'}.{idx}\"\n",
        "                idx += 1\n",
        "                def _mk_hook(k):\n",
        "                    def hook(_, __, out):\n",
        "                        with torch.no_grad():\n",
        "                            t = out\n",
        "                            if isinstance(t, tuple): t = t[0]\n",
        "                            if t is None: return\n",
        "                            t = t.detach()\n",
        "                            s = dict(\n",
        "                                mean=t.mean().item(),\n",
        "                                std=t.std().item(),\n",
        "                                min=t.min().item(),\n",
        "                                max=t.max().item(),\n",
        "                            )\n",
        "                            # % zeros (esp. meaningful after ReLU)\n",
        "                            s[\"pct_zero\"] = (t == 0).float().mean().item()\n",
        "                            self.stats.setdefault(k, s)\n",
        "                    return hook\n",
        "                self.handles.append(m.register_forward_hook(_mk_hook(key)))\n",
        "\n",
        "    def close(self):\n",
        "        for h in self.handles: h.remove()\n",
        "\n",
        "def run_activation_probe(model, batch, device):\n",
        "    model.eval()\n",
        "    x, y = batch\n",
        "    x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "    probe = ActStats(model)\n",
        "    with torch.no_grad():\n",
        "        _ = model(x)\n",
        "    probe.close()\n",
        "    # Pretty print a few key stats\n",
        "    print(\"=== Activation stats (mean/std and % zeros) ===\")\n",
        "    for k, s in list(probe.stats.items()):\n",
        "        print(f\"{k:30s} mean={s['mean']:+.3f} std={s['std']:.3f} \"\n",
        "              f\"min={s['min']:+.3f} max={s['max']:+.3f} %zero={100*s['pct_zero']:.1f}%\")"
      ],
      "metadata": {
        "id": "N9Dni_9Y1vtt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_flow_probe(model, batch, device, criterion=torch.nn.CrossEntropyLoss()):\n",
        "    \"Testing for vanishing and exploding gradients\"\n",
        "    model.train()\n",
        "    (x, y) = batch\n",
        "    x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "\n",
        "    # Zero grads and forward/backward once\n",
        "    for p in model.parameters():\n",
        "        if p.grad is not None: p.grad.zero_()\n",
        "    out = model(x)\n",
        "    loss = criterion(out, y)\n",
        "    loss.backward()\n",
        "\n",
        "    # Collect grad norms and grad-to-weight ratios\n",
        "    rows = []\n",
        "    for name, p in model.named_parameters():\n",
        "        if p.grad is None or not p.requires_grad or not p.is_floating_point():\n",
        "            continue\n",
        "        g = p.grad.detach()\n",
        "        w = p.detach()\n",
        "        grad_norm = g.norm().item()\n",
        "        weight_norm = w.norm().item() + 1e-12\n",
        "        ratio = grad_norm / weight_norm\n",
        "        rows.append((name, grad_norm, weight_norm, ratio))\n",
        "\n",
        "    print(\"=== Gradient flow (per-parameter tensor) ===\")\n",
        "    for (name, g, w, r) in rows:\n",
        "        print(f\"{name:40s} |grad|={g:.3e} |w|={w:.3e} |grad|/|w|={r:.3e}\")\n",
        "\n",
        "    # Heuristic: if early-layer grad norms/ratios are orders of magnitude smaller\n",
        "    # than late layers → vanishing through depth.\n"
      ],
      "metadata": {
        "id": "zGYEtETz1wjP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 1: AlexNet"
      ],
      "metadata": {
        "id": "L2O-MRg2sN0i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With advances in increased data collection with the rise of the internet and availability of GPUs, AlexNet became the state of the art for image recognition in 2012 paving the way for neural networks in many facets of computer vision"
      ],
      "metadata": {
        "id": "PWQJR6aa2Q7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNetCIFAR(nn.Module):\n",
        "    \"\"\"\n",
        "    AlexNet adapted for 32x32 inputs:\n",
        "    - Use 3x3 convs (stride 1) instead of 11x11/5x5\n",
        "    - Slightly reduced channels to fit CIFAR-10 scale\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes: int = 10, dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=7, stride=1, padding=1),  # 32x32 -> 32x32\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),                  # 32 -> 16\n",
        "\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=1),           # 16 -> 16\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),                  # 16 -> 8\n",
        "\n",
        "            nn.Conv2d(192, 384, kernel_size=5, padding=1),          # 8 -> 8\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),          # 8 -> 8\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),          # 8 -> 8\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),                  # 8 -> 4\n",
        "        )\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Conv2d(256, num_classes, kernel_size=1),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        return self.head(x)"
      ],
      "metadata": {
        "id": "2YqKQy4Ysb7Z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AlexNetCIFAR()\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters in AlexNet: {total_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwuC9jSMuJUp",
        "outputId": "3e9cd0f9-3357-41a0-f1fb-75fd08964291"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters in AlexNet: 3638090\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "batch_size = 128\n",
        "lr = 0.001\n",
        "weight_decay = 5e-4\n",
        "num_workers = 2\n",
        "seed = 42"
      ],
      "metadata": {
        "id": "pMh0blLJuNZM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(seed)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "train_loader, test_loader = get_cifar10_loaders(batch_size=batch_size, num_workers=num_workers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4svgWMDuPf5",
        "outputId": "66d83d88-4b6d-4bf4-d948-6a1e21d1ee37"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 70.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_evaluate(model_name='alexnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u721hvhfuRt_",
        "outputId": "3ffd7044-797f-4291-eb04-894fb3135e4f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01/10 | Train Loss 1.8971 Acc 27.16% | Val Loss 1.5593 Acc 41.99% | Best Val Acc 41.99%\n",
            "Epoch 02/10 | Train Loss 1.5592 Acc 42.25% | Val Loss 1.4078 Acc 48.03% | Best Val Acc 48.03%\n",
            "Epoch 03/10 | Train Loss 1.4273 Acc 48.07% | Val Loss 1.2888 Acc 53.55% | Best Val Acc 53.55%\n",
            "Epoch 04/10 | Train Loss 1.3284 Acc 52.38% | Val Loss 1.2618 Acc 54.31% | Best Val Acc 54.31%\n",
            "Epoch 05/10 | Train Loss 1.2729 Acc 54.29% | Val Loss 1.1868 Acc 57.33% | Best Val Acc 57.33%\n",
            "Epoch 06/10 | Train Loss 1.2088 Acc 56.90% | Val Loss 1.1726 Acc 58.25% | Best Val Acc 58.25%\n",
            "Epoch 07/10 | Train Loss 1.1645 Acc 58.86% | Val Loss 1.0700 Acc 61.78% | Best Val Acc 61.78%\n",
            "Epoch 08/10 | Train Loss 1.1129 Acc 60.63% | Val Loss 1.1176 Acc 61.14% | Best Val Acc 61.78%\n",
            "Epoch 09/10 | Train Loss 1.0771 Acc 62.18% | Val Loss 1.0427 Acc 63.53% | Best Val Acc 63.53%\n",
            "Epoch 10/10 | Train Loss 1.0430 Acc 63.50% | Val Loss 0.9926 Acc 64.89% | Best Val Acc 64.89%\n",
            "\n",
            "Final ALEXNET Test Accuracy: 64.89% (loss 0.9926)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 2: VGGNet (without Batch Normalization)"
      ],
      "metadata": {
        "id": "omjjDhqnsjjY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can we improve accuracy by going deeper? Let's replace the large convolutions of alexnet with multiple 3x3 convolutions. Their receptive fields are equivalent."
      ],
      "metadata": {
        "id": "0W5P7QCu2kdm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGGNetCIFAR(nn.Module):\n",
        "    \"\"\"\n",
        "    Replace larger convolutions in alexnet with 3x3 convolutions.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes: int = 10, dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1),  # 32x32 -> 32x32\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),  # 32x32 -> 32x32\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(16, 64, kernel_size=3, stride=1, padding=1),  # 32x32 -> 32x32\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),                  # 32 -> 16\n",
        "\n",
        "            nn.Conv2d(64, 32, kernel_size=3, padding=1),           # 16 -> 16\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 192, kernel_size=3, padding=1),           # 16 -> 16\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),                  # 16 -> 8\n",
        "\n",
        "            nn.Conv2d(192, 64, kernel_size=3, padding=1),          # 8 -> 8\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 384, kernel_size=3, padding=1),          # 8 -> 8\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),          # 8 -> 8\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),          # 8 -> 8\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),                  # 8 -> 4\n",
        "        )\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Conv2d(256, num_classes, kernel_size=1),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        return self.head(x)"
      ],
      "metadata": {
        "id": "PF_hevn-r2xA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VGGNetCIFAR()\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters in VGGNet: {total_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZdDJdidr7gl",
        "outputId": "ad748496-6490-475f-becd-31c026b3a55d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters in VGGNet: 1894490\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "batch_size = 128\n",
        "lr = 0.001\n",
        "weight_decay = 5e-4\n",
        "num_workers = 2\n",
        "seed = 42"
      ],
      "metadata": {
        "id": "Ub_7ZEqIr3IK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(seed)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "train_loader, test_loader = get_cifar10_loaders(batch_size=batch_size, num_workers=num_workers)"
      ],
      "metadata": {
        "id": "gvQbqf9isD0q"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_evaluate(model_name='vggnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qAagsb9sGS-",
        "outputId": "13bdc91e-f126-4034-a28d-887992ca4a6a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01/10 | Train Loss 2.3030 Acc 9.75% | Val Loss 2.3026 Acc 10.00% | Best Val Acc 10.00%\n",
            "Epoch 02/10 | Train Loss 2.3028 Acc 9.97% | Val Loss 2.3026 Acc 10.00% | Best Val Acc 10.00%\n",
            "Epoch 03/10 | Train Loss 2.3028 Acc 9.82% | Val Loss 2.3026 Acc 10.00% | Best Val Acc 10.00%\n",
            "Epoch 04/10 | Train Loss 2.3027 Acc 9.91% | Val Loss 2.3026 Acc 10.00% | Best Val Acc 10.00%\n",
            "Epoch 05/10 | Train Loss 2.3027 Acc 9.75% | Val Loss 2.3026 Acc 10.00% | Best Val Acc 10.00%\n",
            "Epoch 06/10 | Train Loss 2.3027 Acc 9.87% | Val Loss 2.3026 Acc 10.00% | Best Val Acc 10.00%\n",
            "Epoch 07/10 | Train Loss 2.3027 Acc 9.95% | Val Loss 2.3026 Acc 10.00% | Best Val Acc 10.00%\n",
            "Epoch 08/10 | Train Loss 2.3027 Acc 9.72% | Val Loss 2.3026 Acc 10.00% | Best Val Acc 10.00%\n",
            "Epoch 09/10 | Train Loss 2.3027 Acc 9.87% | Val Loss 2.3026 Acc 10.00% | Best Val Acc 10.00%\n",
            "Epoch 10/10 | Train Loss 2.3027 Acc 9.76% | Val Loss 2.3026 Acc 10.00% | Best Val Acc 10.00%\n",
            "\n",
            "Final VGGNET Test Accuracy: 10.00% (loss 2.3026)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like the network is not learning. We can try diagnosing the issue"
      ],
      "metadata": {
        "id": "1keMyl1wtbGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = VGGNetCIFAR().to(device)\n",
        "batch = next(iter(train_loader))"
      ],
      "metadata": {
        "id": "hO7KWaop_bpE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# activation probe (look for %zero ~100% or tiny stds in early layers)\n",
        "run_activation_probe(model, batch, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxNq9vSY203S",
        "outputId": "626d6fc8-84fe-4fc6-f6ce-a663e755d5a0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Activation stats (mean/std and % zeros) ===\n",
            "features.0.0                   mean=-0.048 std=0.546 min=-2.904 max=+2.635 %zero=0.0%\n",
            "features.1.1                   mean=+0.182 std=0.282 min=+0.000 max=+2.635 %zero=49.6%\n",
            "features.2.2                   mean=+0.007 std=0.200 min=-1.052 max=+1.347 %zero=0.0%\n",
            "features.3.3                   mean=+0.075 std=0.132 min=+0.000 max=+1.347 %zero=49.8%\n",
            "features.4.4                   mean=+0.019 std=0.096 min=-0.692 max=+0.585 %zero=0.0%\n",
            "features.5.5                   mean=+0.047 std=0.062 min=+0.000 max=+0.585 %zero=40.5%\n",
            "features.7.6                   mean=+0.007 std=0.057 min=-0.303 max=+0.324 %zero=0.0%\n",
            "features.8.7                   mean=+0.025 std=0.037 min=+0.000 max=+0.324 %zero=47.4%\n",
            "features.9.8                   mean=+0.003 std=0.042 min=-0.173 max=+0.188 %zero=0.0%\n",
            "features.10.9                  mean=+0.019 std=0.025 min=+0.000 max=+0.188 %zero=48.4%\n",
            "features.12.10                 mean=-0.001 std=0.022 min=-0.088 max=+0.085 %zero=0.0%\n",
            "features.13.11                 mean=+0.008 std=0.013 min=+0.000 max=+0.085 %zero=52.7%\n",
            "features.14.12                 mean=-0.001 std=0.026 min=-0.066 max=+0.064 %zero=0.0%\n",
            "features.15.13                 mean=+0.010 std=0.014 min=+0.000 max=+0.064 %zero=51.6%\n",
            "features.16.14                 mean=+0.001 std=0.013 min=-0.040 max=+0.042 %zero=0.0%\n",
            "features.17.15                 mean=+0.006 std=0.008 min=+0.000 max=+0.042 %zero=46.6%\n",
            "features.18.16                 mean=-0.000 std=0.013 min=-0.033 max=+0.032 %zero=0.0%\n",
            "features.19.17                 mean=+0.005 std=0.007 min=+0.000 max=+0.032 %zero=51.1%\n",
            "head.0.18                      mean=-0.006 std=0.030 min=-0.048 max=+0.056 %zero=0.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient flow probe (look for tiny |grad|/|w| early vs late)\n",
        "grad_flow_probe(model, batch, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kab9e3HC_sFq",
        "outputId": "134ff7ac-9e1e-4863-ad85-ec62fc0c92db"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Gradient flow (per-parameter tensor) ===\n",
            "features.0.weight                        |grad|=4.891e-05 |w|=1.593e+00 |grad|/|w|=3.071e-05\n",
            "features.0.bias                          |grad|=1.117e-05 |w|=3.777e-01 |grad|/|w|=2.957e-05\n",
            "features.2.weight                        |grad|=7.280e-05 |w|=2.314e+00 |grad|/|w|=3.146e-05\n",
            "features.2.bias                          |grad|=2.601e-05 |w|=2.399e-01 |grad|/|w|=1.084e-04\n",
            "features.4.weight                        |grad|=1.725e-04 |w|=4.651e+00 |grad|/|w|=3.709e-05\n",
            "features.4.bias                          |grad|=8.614e-05 |w|=3.962e-01 |grad|/|w|=2.174e-04\n",
            "features.7.weight                        |grad|=4.305e-04 |w|=3.258e+00 |grad|/|w|=1.321e-04\n",
            "features.7.bias                          |grad|=1.827e-04 |w|=1.272e-01 |grad|/|w|=1.437e-03\n",
            "features.9.weight                        |grad|=3.993e-04 |w|=8.013e+00 |grad|/|w|=4.983e-05\n",
            "features.9.bias                          |grad|=4.856e-04 |w|=4.828e-01 |grad|/|w|=1.006e-03\n",
            "features.12.weight                       |grad|=1.813e-03 |w|=4.617e+00 |grad|/|w|=3.928e-04\n",
            "features.12.bias                         |grad|=1.273e-03 |w|=1.058e-01 |grad|/|w|=1.203e-02\n",
            "features.14.weight                       |grad|=1.375e-03 |w|=1.131e+01 |grad|/|w|=1.215e-04\n",
            "features.14.bias                         |grad|=4.185e-03 |w|=4.770e-01 |grad|/|w|=8.774e-03\n",
            "features.16.weight                       |grad|=1.045e-02 |w|=9.236e+00 |grad|/|w|=1.131e-03\n",
            "features.16.bias                         |grad|=1.136e-02 |w|=1.592e-01 |grad|/|w|=7.135e-02\n",
            "features.18.weight                       |grad|=1.410e-02 |w|=9.238e+00 |grad|/|w|=1.527e-03\n",
            "features.18.bias                         |grad|=3.391e-02 |w|=1.933e-01 |grad|/|w|=1.754e-01\n",
            "head.0.weight                            |grad|=1.325e-02 |w|=1.848e+00 |grad|/|w|=7.168e-03\n",
            "head.0.bias                              |grad|=8.467e-02 |w|=9.196e-02 |grad|/|w|=9.207e-01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Very clearly, the gradients in the 1st part of the network are much smaller than the later parts of the network. This indicates a **vanishing gradient problem**"
      ],
      "metadata": {
        "id": "m7sHzxMY3WL1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 3: VGGNet + Batch Normalization (to reduce vanishing gradients)"
      ],
      "metadata": {
        "id": "gR94bKW8s5Ly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can address this classic case of vanishing gradients with Batch normalization after every convolution and before the activations. This will normalize the activations of each layer to have a 0 mean and unit variance, improving gradient flow."
      ],
      "metadata": {
        "id": "Bph4lYz73pkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGGNetBNCIFAR(nn.Module):\n",
        "    \"\"\"\n",
        "    Add Batch Normalization\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes: int = 10, dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1),  # 32x32 -> 32x32\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),  # 32x32 -> 32x32\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(16, 64, kernel_size=3, stride=1, padding=1),  # 32x32 -> 32x32\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),                  # 32 -> 16\n",
        "\n",
        "            nn.Conv2d(64, 32, kernel_size=3, padding=1),           # 16 -> 16\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 192, kernel_size=3, padding=1),           # 16 -> 16\n",
        "            nn.BatchNorm2d(192),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),                  # 16 -> 8\n",
        "\n",
        "            nn.Conv2d(192, 64, kernel_size=3, padding=1),          # 8 -> 8\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 384, kernel_size=3, padding=1),          # 8 -> 8\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),          # 8 -> 8\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),          # 8 -> 8\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),                  # 8 -> 4\n",
        "        )\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Conv2d(256, num_classes, kernel_size=1),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        return self.head(x)"
      ],
      "metadata": {
        "id": "OvXQ7DLjsJcj"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VGGNetBNCIFAR()\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters in VGGNet with BN: {total_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPm4NFPbtYzO",
        "outputId": "cf01ea6c-ec1d-4151-e66d-ff214a966c48"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters in VGGNet with BN: 1896522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "batch_size = 128\n",
        "lr = 0.001\n",
        "weight_decay = 5e-4\n",
        "num_workers = 2\n",
        "seed = 42"
      ],
      "metadata": {
        "id": "8961I3k0ws5u"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(seed)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "train_loader, test_loader = get_cifar10_loaders(batch_size=batch_size, num_workers=num_workers)"
      ],
      "metadata": {
        "id": "vXWo6e_IwvHE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_evaluate(model_name='vggnet_batch_norm')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpDVAiygwxkR",
        "outputId": "58bc3073-abd2-4781-c30b-c05d9a8e4930"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01/10 | Train Loss 1.4771 Acc 44.95% | Val Loss 1.4091 Acc 50.13% | Best Val Acc 50.13%\n",
            "Epoch 02/10 | Train Loss 1.0121 Acc 63.93% | Val Loss 1.1309 Acc 61.61% | Best Val Acc 61.61%\n",
            "Epoch 03/10 | Train Loss 0.8328 Acc 70.62% | Val Loss 0.9261 Acc 68.76% | Best Val Acc 68.76%\n",
            "Epoch 04/10 | Train Loss 0.7151 Acc 75.21% | Val Loss 0.8410 Acc 71.72% | Best Val Acc 71.72%\n",
            "Epoch 05/10 | Train Loss 0.6426 Acc 77.78% | Val Loss 0.8647 Acc 71.61% | Best Val Acc 71.72%\n",
            "Epoch 06/10 | Train Loss 0.5923 Acc 79.68% | Val Loss 0.7191 Acc 75.88% | Best Val Acc 75.88%\n",
            "Epoch 07/10 | Train Loss 0.5524 Acc 81.05% | Val Loss 0.6249 Acc 78.82% | Best Val Acc 78.82%\n",
            "Epoch 08/10 | Train Loss 0.5196 Acc 81.95% | Val Loss 0.6250 Acc 79.32% | Best Val Acc 79.32%\n",
            "Epoch 09/10 | Train Loss 0.4972 Acc 83.06% | Val Loss 0.9024 Acc 71.75% | Best Val Acc 79.32%\n",
            "Epoch 10/10 | Train Loss 0.4745 Acc 83.67% | Val Loss 0.5931 Acc 80.32% | Best Val Acc 80.32%\n",
            "\n",
            "Final VGGNET_BATCH_NORM Test Accuracy: 80.32% (loss 0.5931)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = VGGNetBNCIFAR().to(device)\n",
        "batch = next(iter(train_loader))"
      ],
      "metadata": {
        "id": "VzuhwIbk4OrA"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# activation probe (look for %zero ~100% or tiny stds in early layers)\n",
        "run_activation_probe(model, batch, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oj9bSx4e4nWp",
        "outputId": "efbcd623-0841-4a99-ca1f-88b7647eec4f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Activation stats (mean/std and % zeros) ===\n",
            "features.0.0                   mean=+0.016 std=0.731 min=-3.320 max=+3.006 %zero=0.0%\n",
            "features.2.1                   mean=+0.282 std=0.446 min=+0.000 max=+3.096 %zero=49.7%\n",
            "features.3.2                   mean=+0.019 std=0.331 min=-1.727 max=+2.115 %zero=0.0%\n",
            "features.5.3                   mean=+0.127 std=0.195 min=+0.000 max=+2.148 %zero=45.9%\n",
            "features.6.4                   mean=-0.008 std=0.144 min=-0.874 max=+0.903 %zero=0.0%\n",
            "features.8.5                   mean=+0.049 std=0.080 min=+0.000 max=+0.919 %zero=52.9%\n",
            "features.10.6                  mean=-0.001 std=0.061 min=-0.294 max=+0.291 %zero=0.0%\n",
            "features.12.7                  mean=+0.025 std=0.035 min=+0.000 max=+0.322 %zero=46.1%\n",
            "features.13.8                  mean=+0.003 std=0.039 min=-0.185 max=+0.177 %zero=0.0%\n",
            "features.15.9                  mean=+0.016 std=0.022 min=+0.000 max=+0.154 %zero=47.7%\n",
            "features.17.10                 mean=-0.001 std=0.022 min=-0.085 max=+0.100 %zero=0.0%\n",
            "features.19.11                 mean=+0.012 std=0.018 min=+0.000 max=+0.094 %zero=54.3%\n",
            "features.20.12                 mean=+0.001 std=0.027 min=-0.072 max=+0.080 %zero=0.0%\n",
            "features.22.13                 mean=+0.012 std=0.017 min=+0.000 max=+0.113 %zero=50.3%\n",
            "features.23.14                 mean=+0.000 std=0.015 min=-0.051 max=+0.045 %zero=0.0%\n",
            "features.25.15                 mean=+0.009 std=0.013 min=+0.000 max=+0.065 %zero=49.3%\n",
            "features.26.16                 mean=-0.000 std=0.015 min=-0.048 max=+0.040 %zero=0.0%\n",
            "features.27.17                 mean=+0.006 std=0.008 min=+0.000 max=+0.040 %zero=49.8%\n",
            "head.0.18                      mean=+0.018 std=0.038 min=-0.050 max=+0.064 %zero=0.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient flow probe (look for tiny |grad|/|w| early vs late)\n",
        "grad_flow_probe(model, batch, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15XuRsnJ4Waj",
        "outputId": "5bfc2159-9ba0-4692-f71f-07c57f0e0326"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Gradient flow (per-parameter tensor) ===\n",
            "features.0.weight                        |grad|=1.104e-01 |w|=1.607e+00 |grad|/|w|=6.872e-02\n",
            "features.0.bias                          |grad|=6.346e-08 |w|=3.963e-01 |grad|/|w|=1.601e-07\n",
            "features.1.weight                        |grad|=1.124e-02 |w|=2.828e+00 |grad|/|w|=3.974e-03\n",
            "features.1.bias                          |grad|=8.962e-03 |w|=1.000e-12 |grad|/|w|=8.962e+09\n",
            "features.3.weight                        |grad|=2.356e-01 |w|=2.301e+00 |grad|/|w|=1.024e-01\n",
            "features.3.bias                          |grad|=9.564e-08 |w|=2.729e-01 |grad|/|w|=3.505e-07\n",
            "features.4.weight                        |grad|=1.491e-02 |w|=4.000e+00 |grad|/|w|=3.727e-03\n",
            "features.4.bias                          |grad|=1.032e-02 |w|=1.000e-12 |grad|/|w|=1.032e+10\n",
            "features.6.weight                        |grad|=2.555e-01 |w|=4.602e+00 |grad|/|w|=5.552e-02\n",
            "features.6.bias                          |grad|=3.757e-08 |w|=4.095e-01 |grad|/|w|=9.174e-08\n",
            "features.7.weight                        |grad|=9.361e-03 |w|=8.000e+00 |grad|/|w|=1.170e-03\n",
            "features.7.bias                          |grad|=6.477e-03 |w|=1.000e-12 |grad|/|w|=6.477e+09\n",
            "features.10.weight                       |grad|=4.367e-01 |w|=3.268e+00 |grad|/|w|=1.336e-01\n",
            "features.10.bias                         |grad|=2.853e-08 |w|=1.359e-01 |grad|/|w|=2.099e-07\n",
            "features.11.weight                       |grad|=9.187e-03 |w|=5.657e+00 |grad|/|w|=1.624e-03\n",
            "features.11.bias                         |grad|=7.425e-03 |w|=1.000e-12 |grad|/|w|=7.425e+09\n",
            "features.13.weight                       |grad|=2.370e-01 |w|=8.000e+00 |grad|/|w|=2.963e-02\n",
            "features.13.bias                         |grad|=1.480e-08 |w|=4.540e-01 |grad|/|w|=3.260e-08\n",
            "features.14.weight                       |grad|=7.071e-03 |w|=1.386e+01 |grad|/|w|=5.103e-04\n",
            "features.14.bias                         |grad|=4.037e-03 |w|=1.000e-12 |grad|/|w|=4.037e+09\n",
            "features.17.weight                       |grad|=5.216e-01 |w|=4.616e+00 |grad|/|w|=1.130e-01\n",
            "features.17.bias                         |grad|=7.284e-09 |w|=1.132e-01 |grad|/|w|=6.434e-08\n",
            "features.18.weight                       |grad|=6.017e-03 |w|=8.000e+00 |grad|/|w|=7.522e-04\n",
            "features.18.bias                         |grad|=4.941e-03 |w|=1.000e-12 |grad|/|w|=4.941e+09\n",
            "features.20.weight                       |grad|=2.514e-01 |w|=1.130e+01 |grad|/|w|=2.225e-02\n",
            "features.20.bias                         |grad|=8.176e-09 |w|=4.815e-01 |grad|/|w|=1.698e-08\n",
            "features.21.weight                       |grad|=5.923e-03 |w|=1.960e+01 |grad|/|w|=3.023e-04\n",
            "features.21.bias                         |grad|=4.828e-03 |w|=1.000e-12 |grad|/|w|=4.828e+09\n",
            "features.23.weight                       |grad|=5.826e-01 |w|=9.233e+00 |grad|/|w|=6.310e-02\n",
            "features.23.bias                         |grad|=7.522e-09 |w|=1.552e-01 |grad|/|w|=4.846e-08\n",
            "features.24.weight                       |grad|=1.495e-02 |w|=1.600e+01 |grad|/|w|=9.345e-04\n",
            "features.24.bias                         |grad|=1.847e-02 |w|=1.000e-12 |grad|/|w|=1.847e+10\n",
            "features.26.weight                       |grad|=1.219e+00 |w|=9.236e+00 |grad|/|w|=1.320e-01\n",
            "features.26.bias                         |grad|=7.164e-02 |w|=1.972e-01 |grad|/|w|=3.633e-01\n",
            "head.0.weight                            |grad|=8.429e-01 |w|=1.821e+00 |grad|/|w|=4.627e-01\n",
            "head.0.bias                              |grad|=1.429e-01 |w|=1.274e-01 |grad|/|w|=1.122e+00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you notice just the weights, it looks like graidents are of the same magnitude throughout the network. The network is learning."
      ],
      "metadata": {
        "id": "j3A905Cp4eom"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 4: Adding more layers to VGGNet"
      ],
      "metadata": {
        "id": "-_xFApvYt1DH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can we go deeper by adding even more layers to the network?"
      ],
      "metadata": {
        "id": "cjeHGHoo5B5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepVGGNetCIFAR(nn.Module):\n",
        "    \"\"\"\n",
        "    Add more layers to create a deeper network\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes: int = 10, dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1),  # 32x32 -> 32x32\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),  # 32x32 -> 32x32\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(16, 64, kernel_size=3, stride=1, padding=1),  # 32x32 -> 32x32\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),                  # 32 -> 16\n",
        "\n",
        "            nn.Conv2d(64, 32, kernel_size=3, padding=1),           # 16 -> 16\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 192, kernel_size=3, padding=1),           # 16 -> 16\n",
        "            nn.BatchNorm2d(192),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),                  # 16 -> 8\n",
        "\n",
        "            nn.Conv2d(192, 64, kernel_size=3, padding=1),          # 8 -> 8\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 384, kernel_size=3, padding=1),          # 8 -> 8\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "\n",
        "            ###### Add these layers #######\n",
        "            nn.Conv2d(384, 64, kernel_size=3, padding=1),          # 8 -> 8\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 192, kernel_size=3, padding=1),          # 8 -> 8\n",
        "            nn.BatchNorm2d(192),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(192, 64, kernel_size=3, padding=1),          # 8 -> 8\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 384, kernel_size=3, padding=1),          # 8 -> 8\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(384, 64, kernel_size=3, padding=1),          # 8 -> 8\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 192, kernel_size=3, padding=1),          # 8 -> 8\n",
        "            nn.BatchNorm2d(192),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(192, 64, kernel_size=3, padding=1),          # 8 -> 8\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 384, kernel_size=3, padding=1),          # 8 -> 8\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            ###### End layers added ######\n",
        "\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),          # 8 -> 8\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),          # 8 -> 8\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),                  # 8 -> 4\n",
        "        )\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Conv2d(256, num_classes, kernel_size=1),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        return self.head(x)"
      ],
      "metadata": {
        "id": "wIcNXqaGyKzh"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DeepVGGNetCIFAR()\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters in Deep VGGNet with BN: {total_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYqWLZIXy_k2",
        "outputId": "f43e992c-d10f-40c9-ce84-33d682e5509e"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters in Deep VGGNet with BN: 3227850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "batch_size = 128\n",
        "lr = 0.001\n",
        "weight_decay = 5e-4\n",
        "num_workers = 2\n",
        "seed = 42"
      ],
      "metadata": {
        "id": "jcHnSD81zByf"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(seed)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "train_loader, test_loader = get_cifar10_loaders(batch_size=batch_size, num_workers=num_workers)"
      ],
      "metadata": {
        "id": "HwwqTeeozFKE"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_evaluate(model_name='deep_vggnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xg4VEk6LzG0H",
        "outputId": "055146cf-e004-49c9-c418-d139f3a9e44d"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01/10 | Train Loss 1.8102 Acc 29.33% | Val Loss 2.1262 Acc 27.52% | Best Val Acc 27.52%\n",
            "Epoch 02/10 | Train Loss 1.3589 Acc 49.14% | Val Loss 1.4756 Acc 49.22% | Best Val Acc 49.22%\n",
            "Epoch 03/10 | Train Loss 1.1065 Acc 60.38% | Val Loss 1.2567 Acc 54.08% | Best Val Acc 54.08%\n",
            "Epoch 04/10 | Train Loss 0.9466 Acc 66.67% | Val Loss 0.9152 Acc 67.96% | Best Val Acc 67.96%\n",
            "Epoch 05/10 | Train Loss 0.8455 Acc 70.87% | Val Loss 1.0992 Acc 61.62% | Best Val Acc 67.96%\n",
            "Epoch 06/10 | Train Loss 0.7703 Acc 73.34% | Val Loss 0.8577 Acc 70.79% | Best Val Acc 70.79%\n",
            "Epoch 07/10 | Train Loss 0.7051 Acc 75.91% | Val Loss 0.7908 Acc 74.24% | Best Val Acc 74.24%\n",
            "Epoch 08/10 | Train Loss 0.6508 Acc 77.83% | Val Loss 0.6803 Acc 77.39% | Best Val Acc 77.39%\n",
            "Epoch 09/10 | Train Loss 0.6088 Acc 79.40% | Val Loss 0.7643 Acc 74.92% | Best Val Acc 77.39%\n",
            "Epoch 10/10 | Train Loss 0.5745 Acc 80.62% | Val Loss 0.6681 Acc 77.96% | Best Val Acc 77.96%\n",
            "\n",
            "Final DEEP_VGGNET Test Accuracy: 77.96% (loss 0.6681)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show training error increase along with test error increase compared to the VGGNetBN (this is not classical overfitting). This is **performance degradation**. You can try increasing layers like this and will notice performance will be worse than theser layers"
      ],
      "metadata": {
        "id": "NN_X1Cuxt4uu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = DeepVGGNetCIFAR().to(device)\n",
        "batch = next(iter(train_loader))"
      ],
      "metadata": {
        "id": "43vouZtr5T3J"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# activation probe (look for %zero ~100% or tiny stds in early layers)\n",
        "run_activation_probe(model, batch, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqNiZx955eBb",
        "outputId": "d285f02f-600d-4f42-eb97-b4480e7f4181"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Activation stats (mean/std and % zeros) ===\n",
            "features.0.0                   mean=-0.110 std=0.681 min=-2.968 max=+2.684 %zero=0.0%\n",
            "features.2.1                   mean=+0.222 std=0.344 min=+0.000 max=+2.684 %zero=55.2%\n",
            "features.3.2                   mean=-0.031 std=0.241 min=-1.499 max=+0.870 %zero=0.0%\n",
            "features.5.3                   mean=+0.074 std=0.105 min=+0.000 max=+0.870 %zero=49.3%\n",
            "features.6.4                   mean=-0.005 std=0.088 min=-0.532 max=+0.457 %zero=0.0%\n",
            "features.8.5                   mean=+0.033 std=0.050 min=+0.000 max=+0.457 %zero=52.5%\n",
            "features.10.6                  mean=+0.012 std=0.050 min=-0.238 max=+0.228 %zero=0.0%\n",
            "features.12.7                  mean=+0.026 std=0.034 min=+0.000 max=+0.228 %zero=38.8%\n",
            "features.13.8                  mean=+0.001 std=0.040 min=-0.156 max=+0.160 %zero=0.0%\n",
            "features.15.9                  mean=+0.017 std=0.023 min=+0.000 max=+0.160 %zero=48.1%\n",
            "features.17.10                 mean=-0.000 std=0.023 min=-0.073 max=+0.060 %zero=0.0%\n",
            "features.19.11                 mean=+0.010 std=0.013 min=+0.000 max=+0.060 %zero=48.4%\n",
            "features.20.12                 mean=-0.003 std=0.025 min=-0.065 max=+0.056 %zero=0.0%\n",
            "features.22.13                 mean=+0.009 std=0.013 min=+0.000 max=+0.056 %zero=55.3%\n",
            "features.23.14                 mean=+0.001 std=0.014 min=-0.032 max=+0.037 %zero=0.0%\n",
            "features.25.15                 mean=+0.007 std=0.009 min=+0.000 max=+0.037 %zero=48.8%\n",
            "features.26.16                 mean=+0.001 std=0.026 min=-0.049 max=+0.056 %zero=0.0%\n",
            "features.28.17                 mean=+0.012 std=0.015 min=+0.000 max=+0.056 %zero=46.4%\n",
            "features.29.18                 mean=-0.001 std=0.017 min=-0.041 max=+0.037 %zero=0.0%\n",
            "features.31.19                 mean=+0.007 std=0.009 min=+0.000 max=+0.037 %zero=53.2%\n",
            "features.32.20                 mean=+0.001 std=0.026 min=-0.054 max=+0.058 %zero=0.0%\n",
            "features.34.21                 mean=+0.012 std=0.015 min=+0.000 max=+0.058 %zero=49.9%\n",
            "features.35.22                 mean=+0.001 std=0.014 min=-0.036 max=+0.035 %zero=0.0%\n",
            "features.37.23                 mean=+0.006 std=0.008 min=+0.000 max=+0.035 %zero=52.0%\n",
            "features.38.24                 mean=+0.000 std=0.025 min=-0.052 max=+0.055 %zero=0.0%\n",
            "features.40.25                 mean=+0.011 std=0.014 min=+0.000 max=+0.055 %zero=51.0%\n",
            "features.41.26                 mean=-0.001 std=0.017 min=-0.040 max=+0.034 %zero=0.0%\n",
            "features.43.27                 mean=+0.006 std=0.009 min=+0.000 max=+0.034 %zero=52.9%\n",
            "features.44.28                 mean=+0.001 std=0.026 min=-0.059 max=+0.052 %zero=0.0%\n",
            "features.46.29                 mean=+0.011 std=0.014 min=+0.000 max=+0.052 %zero=47.0%\n",
            "features.47.30                 mean=+0.000 std=0.013 min=-0.038 max=+0.047 %zero=0.0%\n",
            "features.49.31                 mean=+0.005 std=0.008 min=+0.000 max=+0.047 %zero=49.5%\n",
            "features.50.32                 mean=-0.000 std=0.013 min=-0.032 max=+0.029 %zero=0.0%\n",
            "features.51.33                 mean=+0.005 std=0.007 min=+0.000 max=+0.029 %zero=52.2%\n",
            "head.0.34                      mean=+0.002 std=0.044 min=-0.061 max=+0.062 %zero=0.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient flow probe (look for tiny |grad|/|w| early vs late)\n",
        "grad_flow_probe(model, batch, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_NiI0FP5g3E",
        "outputId": "83061465-f0cf-4da9-c41f-06ff56ce87fe"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Gradient flow (per-parameter tensor) ===\n",
            "features.0.weight                        |grad|=4.680e-01 |w|=1.614e+00 |grad|/|w|=2.899e-01\n",
            "features.0.bias                          |grad|=2.945e-07 |w|=3.580e-01 |grad|/|w|=8.228e-07\n",
            "features.1.weight                        |grad|=4.419e-02 |w|=2.828e+00 |grad|/|w|=1.562e-02\n",
            "features.1.bias                          |grad|=8.214e-02 |w|=1.000e-12 |grad|/|w|=8.214e+10\n",
            "features.3.weight                        |grad|=7.819e-01 |w|=2.269e+00 |grad|/|w|=3.445e-01\n",
            "features.3.bias                          |grad|=4.360e-07 |w|=2.771e-01 |grad|/|w|=1.573e-06\n",
            "features.4.weight                        |grad|=4.208e-02 |w|=4.000e+00 |grad|/|w|=1.052e-02\n",
            "features.4.bias                          |grad|=3.909e-02 |w|=1.000e-12 |grad|/|w|=3.909e+10\n",
            "features.6.weight                        |grad|=7.989e-01 |w|=4.616e+00 |grad|/|w|=1.730e-01\n",
            "features.6.bias                          |grad|=1.466e-07 |w|=3.364e-01 |grad|/|w|=4.358e-07\n",
            "features.7.weight                        |grad|=3.346e-02 |w|=8.000e+00 |grad|/|w|=4.183e-03\n",
            "features.7.bias                          |grad|=2.174e-02 |w|=1.000e-12 |grad|/|w|=2.174e+10\n",
            "features.10.weight                       |grad|=1.315e+00 |w|=3.260e+00 |grad|/|w|=4.035e-01\n",
            "features.10.bias                         |grad|=6.881e-08 |w|=1.369e-01 |grad|/|w|=5.026e-07\n",
            "features.11.weight                       |grad|=2.339e-02 |w|=5.657e+00 |grad|/|w|=4.135e-03\n",
            "features.11.bias                         |grad|=2.243e-02 |w|=1.000e-12 |grad|/|w|=2.243e+10\n",
            "features.13.weight                       |grad|=7.717e-01 |w|=7.970e+00 |grad|/|w|=9.683e-02\n",
            "features.13.bias                         |grad|=5.374e-08 |w|=4.742e-01 |grad|/|w|=1.133e-07\n",
            "features.14.weight                       |grad|=2.181e-02 |w|=1.386e+01 |grad|/|w|=1.574e-03\n",
            "features.14.bias                         |grad|=1.342e-02 |w|=1.000e-12 |grad|/|w|=1.342e+10\n",
            "features.17.weight                       |grad|=1.665e+00 |w|=4.623e+00 |grad|/|w|=3.601e-01\n",
            "features.17.bias                         |grad|=2.582e-08 |w|=1.156e-01 |grad|/|w|=2.234e-07\n",
            "features.18.weight                       |grad|=1.912e-02 |w|=8.000e+00 |grad|/|w|=2.389e-03\n",
            "features.18.bias                         |grad|=1.573e-02 |w|=1.000e-12 |grad|/|w|=1.573e+10\n",
            "features.20.weight                       |grad|=7.928e-01 |w|=1.131e+01 |grad|/|w|=7.009e-02\n",
            "features.20.bias                         |grad|=2.849e-08 |w|=4.725e-01 |grad|/|w|=6.030e-08\n",
            "features.21.weight                       |grad|=1.672e-02 |w|=1.960e+01 |grad|/|w|=8.535e-04\n",
            "features.21.bias                         |grad|=1.386e-02 |w|=1.000e-12 |grad|/|w|=1.386e+10\n",
            "features.23.weight                       |grad|=1.609e+00 |w|=4.617e+00 |grad|/|w|=3.485e-01\n",
            "features.23.bias                         |grad|=2.250e-08 |w|=8.212e-02 |grad|/|w|=2.740e-07\n",
            "features.24.weight                       |grad|=1.474e-02 |w|=8.000e+00 |grad|/|w|=1.843e-03\n",
            "features.24.bias                         |grad|=1.065e-02 |w|=1.000e-12 |grad|/|w|=1.065e+10\n",
            "features.26.weight                       |grad|=5.680e-01 |w|=8.006e+00 |grad|/|w|=7.095e-02\n",
            "features.26.bias                         |grad|=1.900e-08 |w|=3.427e-01 |grad|/|w|=5.543e-08\n",
            "features.27.weight                       |grad|=1.264e-02 |w|=1.386e+01 |grad|/|w|=9.121e-04\n",
            "features.27.bias                         |grad|=1.021e-02 |w|=1.000e-12 |grad|/|w|=1.021e+10\n",
            "features.29.weight                       |grad|=8.413e-01 |w|=4.602e+00 |grad|/|w|=1.828e-01\n",
            "features.29.bias                         |grad|=1.638e-08 |w|=1.067e-01 |grad|/|w|=1.535e-07\n",
            "features.30.weight                       |grad|=9.952e-03 |w|=8.000e+00 |grad|/|w|=1.244e-03\n",
            "features.30.bias                         |grad|=8.126e-03 |w|=1.000e-12 |grad|/|w|=8.126e+09\n",
            "features.32.weight                       |grad|=4.109e-01 |w|=1.131e+01 |grad|/|w|=3.634e-02\n",
            "features.32.bias                         |grad|=1.407e-08 |w|=4.779e-01 |grad|/|w|=2.945e-08\n",
            "features.33.weight                       |grad|=7.859e-03 |w|=1.960e+01 |grad|/|w|=4.010e-04\n",
            "features.33.bias                         |grad|=6.699e-03 |w|=1.000e-12 |grad|/|w|=6.699e+09\n",
            "features.35.weight                       |grad|=8.596e-01 |w|=4.621e+00 |grad|/|w|=1.860e-01\n",
            "features.35.bias                         |grad|=1.294e-08 |w|=7.155e-02 |grad|/|w|=1.809e-07\n",
            "features.36.weight                       |grad|=7.476e-03 |w|=8.000e+00 |grad|/|w|=9.345e-04\n",
            "features.36.bias                         |grad|=5.501e-03 |w|=1.000e-12 |grad|/|w|=5.501e+09\n",
            "features.38.weight                       |grad|=2.965e-01 |w|=8.014e+00 |grad|/|w|=3.700e-02\n",
            "features.38.bias                         |grad|=1.129e-08 |w|=3.285e-01 |grad|/|w|=3.436e-08\n",
            "features.39.weight                       |grad|=6.215e-03 |w|=1.386e+01 |grad|/|w|=4.485e-04\n",
            "features.39.bias                         |grad|=4.774e-03 |w|=1.000e-12 |grad|/|w|=4.774e+09\n",
            "features.41.weight                       |grad|=4.482e-01 |w|=4.604e+00 |grad|/|w|=9.736e-02\n",
            "features.41.bias                         |grad|=8.730e-09 |w|=1.033e-01 |grad|/|w|=8.450e-08\n",
            "features.42.weight                       |grad|=5.620e-03 |w|=8.000e+00 |grad|/|w|=7.025e-04\n",
            "features.42.bias                         |grad|=4.060e-03 |w|=1.000e-12 |grad|/|w|=4.060e+09\n",
            "features.44.weight                       |grad|=2.352e-01 |w|=1.131e+01 |grad|/|w|=2.080e-02\n",
            "features.44.bias                         |grad|=7.761e-09 |w|=4.796e-01 |grad|/|w|=1.618e-08\n",
            "features.45.weight                       |grad|=5.248e-03 |w|=1.960e+01 |grad|/|w|=2.678e-04\n",
            "features.45.bias                         |grad|=3.754e-03 |w|=1.000e-12 |grad|/|w|=3.754e+09\n",
            "features.47.weight                       |grad|=5.695e-01 |w|=9.238e+00 |grad|/|w|=6.164e-02\n",
            "features.47.bias                         |grad|=6.604e-09 |w|=1.522e-01 |grad|/|w|=4.339e-08\n",
            "features.48.weight                       |grad|=1.020e-02 |w|=1.600e+01 |grad|/|w|=6.374e-04\n",
            "features.48.bias                         |grad|=1.311e-02 |w|=1.000e-12 |grad|/|w|=1.311e+10\n",
            "features.50.weight                       |grad|=8.654e-01 |w|=9.229e+00 |grad|/|w|=9.377e-02\n",
            "features.50.bias                         |grad|=4.904e-02 |w|=1.945e-01 |grad|/|w|=2.522e-01\n",
            "head.0.weight                            |grad|=5.790e-01 |w|=1.794e+00 |grad|/|w|=3.228e-01\n",
            "head.0.bias                              |grad|=1.028e-01 |w|=1.377e-01 |grad|/|w|=7.467e-01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There doesn't seem to be issues of dying reLU or vanishing gradients."
      ],
      "metadata": {
        "id": "igC3T6Ei6Avo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 5: ResNet: Add residual connections to the same network"
      ],
      "metadata": {
        "id": "epoWUq8et9QI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Minimal residual block; needed so we can do x + F(x) inside a Sequential\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, in_ch, mid_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_ch, mid_ch, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn1   = nn.BatchNorm2d(mid_ch)\n",
        "        self.conv2 = nn.Conv2d(mid_ch, out_ch, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2   = nn.BatchNorm2d(out_ch)\n",
        "        self.proj  = None\n",
        "\n",
        "        if in_ch != out_ch:\n",
        "            self.proj = nn.Sequential(\n",
        "                nn.Conv2d(in_ch, out_ch, kernel_size=1, bias=False),\n",
        "                nn.BatchNorm2d(out_ch),\n",
        "            )\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        if self.proj is not None:\n",
        "            identity = self.proj(identity)\n",
        "        out = self.relu(out + identity)\n",
        "        return out\n",
        "\n",
        "class ResNetCIFAR(nn.Module):\n",
        "    def __init__(self, num_classes: int = 10, dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        # Everything (stem + residual stacks + pools) is inside ONE Sequential\n",
        "        self.features = nn.Sequential(\n",
        "            # Stem (kept simple like your original)\n",
        "            nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(16, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # Residual stacks (your channel plan, now with skips)\n",
        "            Residual(64, 32, 192),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),   # 16 -> 8\n",
        "\n",
        "            Residual(192, 64, 384),\n",
        "\n",
        "            # Your “added” layers as residual pairs\n",
        "            Residual(384, 64, 192),\n",
        "            Residual(192, 64, 384),\n",
        "            Residual(384, 64, 192),\n",
        "            Residual(192, 64, 384),\n",
        "\n",
        "            # Tail before final stage\n",
        "            Residual(384, 256, 256),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),   # 8 -> 4\n",
        "        )\n",
        "\n",
        "        # Head kept identical to your style\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Conv2d(256, num_classes, kernel_size=1, bias=True),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        # Kaiming init + BN defaults\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.head(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "S1TsszlHuAOI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNetCIFAR()\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters in Deep ResNet: {total_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMW4VO4ozdwf",
        "outputId": "fff3b71e-e7ec-409d-ddda-444afa40cb49"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters in Deep ResNet: 3727474\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "batch_size = 128\n",
        "lr = 0.001\n",
        "weight_decay = 5e-4\n",
        "num_workers = 2\n",
        "seed = 42"
      ],
      "metadata": {
        "id": "hiQm8Kz3zkq7"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(seed)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "train_loader, test_loader = get_cifar10_loaders(batch_size=batch_size, num_workers=num_workers)"
      ],
      "metadata": {
        "id": "he4jMA30zq-9"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_evaluate(model_name='resnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4N1KzdPDzypl",
        "outputId": "c2db5c53-1ff0-4d15-8e13-56be1f512ce4"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01/10 | Train Loss 1.3642 Acc 50.67% | Val Loss 1.2370 Acc 56.70% | Best Val Acc 56.70%\n",
            "Epoch 02/10 | Train Loss 0.9287 Acc 66.86% | Val Loss 0.9092 Acc 68.70% | Best Val Acc 68.70%\n",
            "Epoch 03/10 | Train Loss 0.7638 Acc 73.24% | Val Loss 1.1951 Acc 62.42% | Best Val Acc 68.70%\n",
            "Epoch 04/10 | Train Loss 0.6807 Acc 76.34% | Val Loss 0.7045 Acc 76.10% | Best Val Acc 76.10%\n",
            "Epoch 05/10 | Train Loss 0.6215 Acc 78.54% | Val Loss 0.6758 Acc 77.56% | Best Val Acc 77.56%\n",
            "Epoch 06/10 | Train Loss 0.5808 Acc 80.12% | Val Loss 0.7190 Acc 76.13% | Best Val Acc 77.56%\n",
            "Epoch 07/10 | Train Loss 0.5456 Acc 81.40% | Val Loss 0.7783 Acc 74.31% | Best Val Acc 77.56%\n",
            "Epoch 08/10 | Train Loss 0.5182 Acc 82.49% | Val Loss 0.7440 Acc 75.97% | Best Val Acc 77.56%\n",
            "Epoch 09/10 | Train Loss 0.4967 Acc 82.91% | Val Loss 0.6441 Acc 78.62% | Best Val Acc 78.62%\n",
            "Epoch 10/10 | Train Loss 0.4654 Acc 84.12% | Val Loss 0.5578 Acc 81.10% | Best Val Acc 81.10%\n",
            "\n",
            "Final RESNET Test Accuracy: 81.10% (loss 0.5578)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = ResNetCIFAR().to(device)\n",
        "batch = next(iter(train_loader))"
      ],
      "metadata": {
        "id": "O-aqb3b0z2Xv"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# activation probe (look for %zero ~100% or tiny stds in early layers)\n",
        "run_activation_probe(model, batch, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58eU7RI-6-OJ",
        "outputId": "2d809f4b-8f65-44bd-d091-007705d5142e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Activation stats (mean/std and % zeros) ===\n",
            "features.0.0                   mean=+0.097 std=1.260 min=-9.333 max=+9.075 %zero=0.0%\n",
            "features.2.1                   mean=+0.496 std=0.846 min=+0.000 max=+9.075 %zero=48.6%\n",
            "features.3.2                   mean=-0.210 std=1.283 min=-12.405 max=+7.836 %zero=0.0%\n",
            "features.5.3                   mean=+0.323 std=0.613 min=+0.000 max=+7.836 %zero=54.7%\n",
            "features.6.4                   mean=+0.076 std=0.933 min=-7.237 max=+8.217 %zero=0.0%\n",
            "features.8.5                   mean=+0.376 std=0.593 min=+0.000 max=+8.217 %zero=45.7%\n",
            "features.9.conv1.6             mean=-0.073 std=0.947 min=-6.405 max=+6.142 %zero=0.0%\n",
            "features.9.relu.8              mean=+0.311 std=0.527 min=+0.000 max=+6.142 %zero=52.9%\n",
            "features.9.conv2.7             mean=-0.058 std=0.794 min=-6.522 max=+6.761 %zero=0.0%\n",
            "features.11.conv1.9            mean=+0.287 std=1.490 min=-8.358 max=+12.045 %zero=0.0%\n",
            "features.11.relu.12            mean=+0.689 std=1.088 min=+0.000 max=+12.045 %zero=44.3%\n",
            "features.11.conv2.10           mean=+0.097 std=1.810 min=-11.475 max=+11.571 %zero=0.0%\n",
            "features.11.proj.0.11          mean=+0.026 std=1.748 min=-19.024 max=+13.798 %zero=0.0%\n",
            "features.13.conv1.13           mean=-0.002 std=2.789 min=-13.824 max=+14.338 %zero=0.0%\n",
            "features.13.relu.16            mean=+1.083 std=1.677 min=+0.000 max=+14.338 %zero=51.0%\n",
            "features.13.conv2.14           mean=-0.073 std=2.699 min=-18.448 max=+17.007 %zero=0.0%\n",
            "features.13.proj.0.15          mean=+0.005 std=3.282 min=-22.244 max=+22.218 %zero=0.0%\n",
            "features.14.conv1.17           mean=-0.085 std=4.061 min=-19.143 max=+21.513 %zero=0.0%\n",
            "features.14.relu.20            mean=+1.512 std=2.366 min=+0.000 max=+21.513 %zero=49.9%\n",
            "features.14.conv2.18           mean=+0.170 std=3.675 min=-22.544 max=+24.431 %zero=0.0%\n",
            "features.14.proj.0.19          mean=-0.518 std=4.265 min=-26.544 max=+22.017 %zero=0.0%\n",
            "features.15.conv1.21           mean=+0.207 std=4.920 min=-29.397 max=+29.159 %zero=0.0%\n",
            "features.15.relu.24            mean=+1.916 std=2.973 min=+0.000 max=+29.159 %zero=48.0%\n",
            "features.15.conv2.22           mean=+0.292 std=4.501 min=-28.919 max=+24.980 %zero=0.0%\n",
            "features.15.proj.0.23          mean=-0.179 std=5.136 min=-33.774 max=+30.624 %zero=0.0%\n",
            "features.16.conv1.25           mean=+0.238 std=6.803 min=-33.528 max=+29.253 %zero=0.0%\n",
            "features.16.relu.28            mean=+2.803 std=4.111 min=+0.000 max=+29.252 %zero=49.2%\n",
            "features.16.conv2.26           mean=-0.276 std=6.902 min=-32.519 max=+38.843 %zero=0.0%\n",
            "features.16.proj.0.27          mean=-0.413 std=6.952 min=-51.887 max=+47.197 %zero=0.0%\n",
            "features.17.conv1.29           mean=+0.109 std=9.060 min=-44.998 max=+50.058 %zero=0.0%\n",
            "features.17.relu.32            mean=+3.556 std=5.537 min=+0.000 max=+50.058 %zero=50.7%\n",
            "features.17.conv2.30           mean=-0.930 std=9.086 min=-64.337 max=+58.008 %zero=0.0%\n",
            "features.17.proj.0.31          mean=-0.479 std=9.340 min=-59.147 max=+60.402 %zero=0.0%\n",
            "features.18.conv1.33           mean=-0.420 std=11.540 min=-65.442 max=+67.639 %zero=0.0%\n",
            "features.18.relu.36            mean=+4.212 std=6.710 min=+0.000 max=+67.638 %zero=51.4%\n",
            "features.18.conv2.34           mean=+0.576 std=10.755 min=-65.577 max=+53.810 %zero=0.0%\n",
            "features.18.proj.0.35          mean=-1.027 std=11.928 min=-94.536 max=+73.294 %zero=0.0%\n",
            "head.0.37                      mean=+0.660 std=12.453 min=-62.649 max=+49.888 %zero=0.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient flow probe (look for tiny |grad|/|w| early vs late)\n",
        "grad_flow_probe(model, batch, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDZH08Jj9wIO",
        "outputId": "47af0bb2-f31e-4535-888e-07a4ff4cf963"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Gradient flow (per-parameter tensor) ===\n",
            "features.0.weight                        |grad|=3.167e+00 |w|=4.164e+00 |grad|/|w|=7.604e-01\n",
            "features.1.weight                        |grad|=3.899e-01 |w|=2.828e+00 |grad|/|w|=1.378e-01\n",
            "features.1.bias                          |grad|=3.612e-01 |w|=1.000e-12 |grad|/|w|=3.612e+11\n",
            "features.3.weight                        |grad|=2.824e+00 |w|=5.721e+00 |grad|/|w|=4.936e-01\n",
            "features.4.weight                        |grad|=4.471e-01 |w|=4.000e+00 |grad|/|w|=1.118e-01\n",
            "features.4.bias                          |grad|=4.376e-01 |w|=1.000e-12 |grad|/|w|=4.376e+11\n",
            "features.6.weight                        |grad|=3.050e+00 |w|=1.150e+01 |grad|/|w|=2.651e-01\n",
            "features.7.weight                        |grad|=2.642e-01 |w|=8.000e+00 |grad|/|w|=3.302e-02\n",
            "features.7.bias                          |grad|=2.204e-01 |w|=1.000e-12 |grad|/|w|=2.204e+11\n",
            "features.9.conv1.weight                  |grad|=4.437e+00 |w|=5.589e+00 |grad|/|w|=7.938e-01\n",
            "features.9.bn1.weight                    |grad|=2.146e-01 |w|=4.000e+00 |grad|/|w|=5.365e-02\n",
            "features.9.bn1.bias                      |grad|=1.805e-01 |w|=1.000e-12 |grad|/|w|=1.805e+11\n",
            "features.9.conv2.weight                  |grad|=1.785e+00 |w|=1.118e+01 |grad|/|w|=1.596e-01\n",
            "features.9.bn2.weight                    |grad|=1.850e-01 |w|=8.000e+00 |grad|/|w|=2.312e-02\n",
            "features.9.bn2.bias                      |grad|=9.220e-02 |w|=1.000e-12 |grad|/|w|=9.220e+10\n",
            "features.11.conv1.weight                 |grad|=3.052e+00 |w|=7.988e+00 |grad|/|w|=3.821e-01\n",
            "features.11.bn1.weight                   |grad|=1.531e-01 |w|=5.657e+00 |grad|/|w|=2.707e-02\n",
            "features.11.bn1.bias                     |grad|=1.244e-01 |w|=1.000e-12 |grad|/|w|=1.244e+11\n",
            "features.11.conv2.weight                 |grad|=1.774e+00 |w|=1.965e+01 |grad|/|w|=9.031e-02\n",
            "features.11.bn2.weight                   |grad|=1.447e-01 |w|=1.386e+01 |grad|/|w|=1.045e-02\n",
            "features.11.bn2.bias                     |grad|=7.323e-02 |w|=1.000e-12 |grad|/|w|=7.323e+10\n",
            "features.11.proj.0.weight                |grad|=8.649e-01 |w|=1.977e+01 |grad|/|w|=4.375e-02\n",
            "features.11.proj.1.weight                |grad|=1.327e-01 |w|=1.386e+01 |grad|/|w|=9.575e-03\n",
            "features.11.proj.1.bias                  |grad|=7.323e-02 |w|=1.000e-12 |grad|/|w|=7.323e+10\n",
            "features.13.conv1.weight                 |grad|=3.947e+00 |w|=1.133e+01 |grad|/|w|=3.484e-01\n",
            "features.13.bn1.weight                   |grad|=1.233e-01 |w|=8.000e+00 |grad|/|w|=1.541e-02\n",
            "features.13.bn1.bias                     |grad|=9.568e-02 |w|=1.000e-12 |grad|/|w|=9.568e+10\n",
            "features.13.conv2.weight                 |grad|=1.934e+00 |w|=2.767e+01 |grad|/|w|=6.988e-02\n",
            "features.13.bn2.weight                   |grad|=1.078e-01 |w|=1.960e+01 |grad|/|w|=5.499e-03\n",
            "features.13.bn2.bias                     |grad|=8.096e-02 |w|=1.000e-12 |grad|/|w|=8.096e+10\n",
            "features.13.proj.0.weight                |grad|=1.122e+00 |w|=2.764e+01 |grad|/|w|=4.060e-02\n",
            "features.13.proj.1.weight                |grad|=1.051e-01 |w|=1.960e+01 |grad|/|w|=5.362e-03\n",
            "features.13.proj.1.bias                  |grad|=8.096e-02 |w|=1.000e-12 |grad|/|w|=8.096e+10\n",
            "features.14.conv1.weight                 |grad|=4.251e+00 |w|=1.133e+01 |grad|/|w|=3.751e-01\n",
            "features.14.bn1.weight                   |grad|=7.573e-02 |w|=8.000e+00 |grad|/|w|=9.466e-03\n",
            "features.14.bn1.bias                     |grad|=7.504e-02 |w|=1.000e-12 |grad|/|w|=7.504e+10\n",
            "features.14.conv2.weight                 |grad|=1.464e+00 |w|=1.952e+01 |grad|/|w|=7.497e-02\n",
            "features.14.bn2.weight                   |grad|=8.238e-02 |w|=1.386e+01 |grad|/|w|=5.945e-03\n",
            "features.14.bn2.bias                     |grad|=6.468e-02 |w|=1.000e-12 |grad|/|w|=6.468e+10\n",
            "features.14.proj.0.weight                |grad|=1.178e+00 |w|=1.965e+01 |grad|/|w|=5.997e-02\n",
            "features.14.proj.1.weight                |grad|=7.697e-02 |w|=1.386e+01 |grad|/|w|=5.554e-03\n",
            "features.14.proj.1.bias                  |grad|=6.468e-02 |w|=1.000e-12 |grad|/|w|=6.468e+10\n",
            "features.15.conv1.weight                 |grad|=2.244e+00 |w|=1.133e+01 |grad|/|w|=1.982e-01\n",
            "features.15.bn1.weight                   |grad|=6.315e-02 |w|=8.000e+00 |grad|/|w|=7.894e-03\n",
            "features.15.bn1.bias                     |grad|=5.989e-02 |w|=1.000e-12 |grad|/|w|=5.989e+10\n",
            "features.15.conv2.weight                 |grad|=1.149e+00 |w|=2.763e+01 |grad|/|w|=4.160e-02\n",
            "features.15.bn2.weight                   |grad|=6.014e-02 |w|=1.960e+01 |grad|/|w|=3.069e-03\n",
            "features.15.bn2.bias                     |grad|=4.840e-02 |w|=1.000e-12 |grad|/|w|=4.840e+10\n",
            "features.15.proj.0.weight                |grad|=6.435e-01 |w|=2.777e+01 |grad|/|w|=2.317e-02\n",
            "features.15.proj.1.weight                |grad|=6.035e-02 |w|=1.960e+01 |grad|/|w|=3.080e-03\n",
            "features.15.proj.1.bias                  |grad|=4.840e-02 |w|=1.000e-12 |grad|/|w|=4.840e+10\n",
            "features.16.conv1.weight                 |grad|=2.538e+00 |w|=1.133e+01 |grad|/|w|=2.241e-01\n",
            "features.16.bn1.weight                   |grad|=5.574e-02 |w|=8.000e+00 |grad|/|w|=6.968e-03\n",
            "features.16.bn1.bias                     |grad|=4.863e-02 |w|=1.000e-12 |grad|/|w|=4.863e+10\n",
            "features.16.conv2.weight                 |grad|=9.358e-01 |w|=1.958e+01 |grad|/|w|=4.779e-02\n",
            "features.16.bn2.weight                   |grad|=5.437e-02 |w|=1.386e+01 |grad|/|w|=3.924e-03\n",
            "features.16.bn2.bias                     |grad|=4.486e-02 |w|=1.000e-12 |grad|/|w|=4.486e+10\n",
            "features.16.proj.0.weight                |grad|=7.795e-01 |w|=1.955e+01 |grad|/|w|=3.987e-02\n",
            "features.16.proj.1.weight                |grad|=5.729e-02 |w|=1.386e+01 |grad|/|w|=4.135e-03\n",
            "features.16.proj.1.bias                  |grad|=4.486e-02 |w|=1.000e-12 |grad|/|w|=4.486e+10\n",
            "features.17.conv1.weight                 |grad|=1.499e+00 |w|=1.129e+01 |grad|/|w|=1.329e-01\n",
            "features.17.bn1.weight                   |grad|=5.328e-02 |w|=8.000e+00 |grad|/|w|=6.661e-03\n",
            "features.17.bn1.bias                     |grad|=4.412e-02 |w|=1.000e-12 |grad|/|w|=4.412e+10\n",
            "features.17.conv2.weight                 |grad|=8.667e-01 |w|=2.773e+01 |grad|/|w|=3.126e-02\n",
            "features.17.bn2.weight                   |grad|=5.109e-02 |w|=1.960e+01 |grad|/|w|=2.607e-03\n",
            "features.17.bn2.bias                     |grad|=3.867e-02 |w|=1.000e-12 |grad|/|w|=3.867e+10\n",
            "features.17.proj.0.weight                |grad|=5.018e-01 |w|=2.774e+01 |grad|/|w|=1.809e-02\n",
            "features.17.proj.1.weight                |grad|=5.379e-02 |w|=1.960e+01 |grad|/|w|=2.745e-03\n",
            "features.17.proj.1.bias                  |grad|=3.867e-02 |w|=1.000e-12 |grad|/|w|=3.867e+10\n",
            "features.18.conv1.weight                 |grad|=1.935e+00 |w|=2.263e+01 |grad|/|w|=8.549e-02\n",
            "features.18.bn1.weight                   |grad|=5.169e-02 |w|=1.600e+01 |grad|/|w|=3.231e-03\n",
            "features.18.bn1.bias                     |grad|=4.047e-02 |w|=1.000e-12 |grad|/|w|=4.047e+10\n",
            "features.18.conv2.weight                 |grad|=1.713e+00 |w|=2.261e+01 |grad|/|w|=7.573e-02\n",
            "features.18.bn2.weight                   |grad|=4.264e-01 |w|=1.600e+01 |grad|/|w|=2.665e-02\n",
            "features.18.bn2.bias                     |grad|=5.667e-01 |w|=1.000e-12 |grad|/|w|=5.667e+11\n",
            "features.18.proj.0.weight                |grad|=7.460e-01 |w|=2.267e+01 |grad|/|w|=3.291e-02\n",
            "features.18.proj.1.weight                |grad|=4.288e-01 |w|=1.600e+01 |grad|/|w|=2.680e-02\n",
            "features.18.proj.1.bias                  |grad|=5.667e-01 |w|=1.000e-12 |grad|/|w|=5.667e+11\n",
            "head.0.weight                            |grad|=9.822e+00 |w|=4.454e+00 |grad|/|w|=2.205e+00\n",
            "head.0.bias                              |grad|=4.559e-01 |w|=1.000e-12 |grad|/|w|=4.559e+11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 6: Deeper than deep"
      ],
      "metadata": {
        "id": "MHKBaRVy_Xxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNetV2CIFAR(nn.Module):\n",
        "    def __init__(self, num_classes: int = 10, dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        # Everything (stem + residual stacks + pools) is inside ONE Sequential\n",
        "        self.features = nn.Sequential(\n",
        "            # Stem (kept simple like your original)\n",
        "            nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(16, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # Residual stacks (your channel plan, now with skips)\n",
        "            Residual(64, 16, 64),          # (was 8->16->64 earlier; now residual at 64)\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),   # 32 -> 16\n",
        "\n",
        "            Residual(64, 32, 192),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),   # 16 -> 8\n",
        "\n",
        "            Residual(192, 64, 384),\n",
        "\n",
        "            # “added” layers as residual pairs\n",
        "            Residual(384, 64, 192),\n",
        "            Residual(192, 64, 384),\n",
        "            Residual(384, 64, 192),\n",
        "            Residual(192, 64, 384),\n",
        "\n",
        "\n",
        "            # Added layers\n",
        "            Residual(384, 64, 384),\n",
        "            Residual(384, 64, 384),\n",
        "            Residual(384, 64, 384),\n",
        "            Residual(384, 64, 384),\n",
        "            Residual(384, 64, 384),\n",
        "\n",
        "            # Tail before final stage\n",
        "            Residual(384, 256, 256),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),   # 8 -> 4\n",
        "        )\n",
        "\n",
        "        # Head kept identical to your style\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Conv2d(256, num_classes, kernel_size=1, bias=True),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        # Kaiming init + BN defaults\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.head(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "FzxdvGbm_aBN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNetV2CIFAR()\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters in Deep ResNet: {total_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1l6jqq__1ma",
        "outputId": "192c31e3-019d-4de5-cf72-8f87fbe8ceed"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters in Deep ResNet: 5943794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "batch_size = 128\n",
        "lr = 0.001\n",
        "weight_decay = 5e-4\n",
        "num_workers = 2\n",
        "seed = 42"
      ],
      "metadata": {
        "id": "Z2LFunkj_70K"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(seed)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "train_loader, test_loader = get_cifar10_loaders(batch_size=batch_size, num_workers=num_workers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M14N0ooy_9x8",
        "outputId": "fa43a59a-0feb-4763-d21e-b381f1caf1af"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 38.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_evaluate(model_name='resnet_v2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbPSSumJ__jv",
        "outputId": "f81f4fa7-5109-41e5-9d4b-a549e47173bb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01/10 | Train Loss 1.4600 Acc 47.30% | Val Loss 1.3920 Acc 52.20% | Best Val Acc 52.20%\n",
            "Epoch 02/10 | Train Loss 0.9947 Acc 64.98% | Val Loss 1.0860 Acc 61.74% | Best Val Acc 61.74%\n",
            "Epoch 03/10 | Train Loss 0.8097 Acc 71.61% | Val Loss 0.9128 Acc 69.12% | Best Val Acc 69.12%\n",
            "Epoch 04/10 | Train Loss 0.7061 Acc 75.67% | Val Loss 0.9715 Acc 67.66% | Best Val Acc 69.12%\n",
            "Epoch 05/10 | Train Loss 0.6398 Acc 77.95% | Val Loss 0.6740 Acc 77.13% | Best Val Acc 77.13%\n",
            "Epoch 06/10 | Train Loss 0.5836 Acc 80.00% | Val Loss 0.6718 Acc 77.28% | Best Val Acc 77.28%\n",
            "Epoch 07/10 | Train Loss 0.5486 Acc 81.44% | Val Loss 0.6752 Acc 77.96% | Best Val Acc 77.96%\n",
            "Epoch 08/10 | Train Loss 0.5161 Acc 82.36% | Val Loss 0.6665 Acc 77.77% | Best Val Acc 77.96%\n",
            "Epoch 09/10 | Train Loss 0.4832 Acc 83.56% | Val Loss 0.5690 Acc 81.13% | Best Val Acc 81.13%\n",
            "Epoch 10/10 | Train Loss 0.4620 Acc 84.06% | Val Loss 0.5663 Acc 81.27% | Best Val Acc 81.27%\n",
            "\n",
            "Final RESNET_V2 Test Accuracy: 81.27% (loss 0.5663)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "- Making a neural network deep can lead to issues like the vanishing gradient problem where the network does not learn\n",
        "- This can be solved with Batch Normalization.\n",
        "- But as we continue to add layers to the network, there is another problem that arises: performance degradation.\n",
        "- This is strange as in theory, deeper networks should be able to mimic shallow networks with some layers acting as a passthrough / identity. This however does not happen in practice as it is difficult for Conv + BN + ReLU to emulate a pass through.\n",
        "- To solve this, we change the structure of the network by adding residual connections. So the resnet block is H(x) = ReLU(x + F(x)). And now, H(x) can more easily mimic a passthrough. This means F(x) which is the Conv + BN + Pooling now has to be 0. This can be done in multiple ways (like having all convolution parameters be 0, batch norm parameters learn to be 0)\n",
        "- This should ensure in theory, a deeper network is at least as performance as a shallow network. We saw this true in practice with Model 6."
      ],
      "metadata": {
        "id": "ShpWj__8GvAs"
      }
    }
  ]
}